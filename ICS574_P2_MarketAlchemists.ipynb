{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Course: ICS 574\n",
        "# Prject: Alchemists Project - EDA\n",
        "- Date: May 1st 2024\n",
        "- Project Members\n",
        "    - AHMED DHAFER ALQARNI, ID: 201453160\n",
        "    - WALEED ABDULLAH ALFAIFI, ID: 201640920\n",
        "    - ALGHAMDI, BANDAR, ID: 202206560"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The project is divided into 6 parts\n",
        "1. Importing the libraries\n",
        "2. Preprocessing the data\n",
        "3. Model Selection\n",
        "\n",
        "    3.1 Decision Tree Classifier Model\n",
        "\n",
        "    3.2 Decision Tree Regressor Model\n",
        "\n",
        "    3.3 GBTRegressor Model\n",
        "\n",
        "    3.4 Random Forest Regressor Model\n",
        "\n",
        "    3.5 Support Vector Classifier Model\n",
        "\n",
        "    3.6 Support Vector Regressor Model\n",
        "    \n",
        "4. Model Evaluation\n",
        "\n",
        "   \n",
        "    \n",
        "5. Model Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://192.168.100.154:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>MarketAlchemists</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x13baa9e90>"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import needed libraries\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"MarketAlchemists\").getOrCreate()\n",
        "spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Preprocess the stock data as done in the EDA Part previously we have seen that the data is not clean and has missing values in Phase 1 of the project\n",
        "- below code is to import the data, clean it, rename columns, and merge the dataframes as done in the EDA Phase 1 part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the datasets as pandas DataFrames\n",
        "stocks_df = pd.read_csv(\"./datasets/Tadawul_stcks.csv\")\n",
        "brent_df = pd.read_csv(\"./datasets/BrentOilPrices.csv\")\n",
        "gold_df = pd.read_csv(\"./datasets/Gold_Daily.csv\")\n",
        "\n",
        "# hide warnings for better readability\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Preprocess the stock data as done in the EDA Part\n",
        "# previously we have seen that the data is not clean and has missing values in Phase 1 of the project\n",
        "# below code is to clean the data, rename the columns and merge the dataframes\n",
        "# as done in the EDA Phase 1 part\n",
        "stocks_specific_df = stocks_df[(stocks_df['sectoer'] == 'Energy') | (stocks_df['sectoer'] == 'Materials')]\n",
        "stocks_specific_df.rename(columns={'date': 'Date'}, inplace=True)\n",
        "stocks_specific_df.rename(columns={'close': 'Stock_Price'}, inplace=True)\n",
        "stocks_specific_df['Date'] = pd.to_datetime(stocks_specific_df['Date'])\n",
        "brent_df['Date'] = pd.to_datetime(brent_df['Date'])\n",
        "gold_df['Date'] = pd.to_datetime(gold_df['Date'])\n",
        "brent_df.rename(columns={'Price': 'Brent_Price'}, inplace=True)\n",
        "gold_df.rename(columns={'Price': 'Gold_Price'}, inplace=True)\n",
        "stocks_specific_df.rename(columns={'sectoer': 'Sector'}, inplace=True)\n",
        "brent_df = brent_df[['Date', 'Brent_Price']]\n",
        "gold_df = gold_df[['Date', 'Gold_Price']]\n",
        "stocks_specific_df.fillna(method='ffill', inplace=True)\n",
        "stocks_brent_df = pd.merge(stocks_specific_df, brent_df, on='Date', how='inner')\n",
        "stocks_brent_gold_df = pd.merge(stocks_brent_df, gold_df, on='Date', how='inner')\n",
        "\n",
        "# define a function that does all the above data preprocessing steps so we can use it before we try different models with original data\n",
        "def preprocess_data(stocks_df, brent_df, gold_df):\n",
        "    stocks_specific_df = stocks_df[(stocks_df['sectoer'] == 'Energy') | (stocks_df['sectoer'] == 'Materials')]\n",
        "    stocks_specific_df.rename(columns={'date': 'Date'}, inplace=True)\n",
        "    stocks_specific_df.rename(columns={'close': 'Stock_Price'}, inplace=True)\n",
        "    stocks_specific_df['Date'] = pd.to_datetime(stocks_specific_df['Date'])\n",
        "    brent_df['Date'] = pd.to_datetime(brent_df['Date'])\n",
        "    gold_df['Date'] = pd.to_datetime(gold_df['Date'])\n",
        "    brent_df.rename(columns={'Price': 'Brent_Price'}, inplace=True)\n",
        "    gold_df.rename(columns={'Price': 'Gold_Price'}, inplace=True)\n",
        "    stocks_specific_df.rename(columns={'sectoer': 'Sector'}, inplace=True)\n",
        "    brent_df = brent_df[['Date', 'Brent_Price']]\n",
        "    gold_df = gold_df[['Date', 'Gold_Price']]\n",
        "    stocks_specific_df.fillna(method='ffill', inplace=True)\n",
        "    stocks_brent_df = pd.merge(stocks_specific_df, brent_df, on='Date', how='inner')\n",
        "    stocks_brent_gold_df = pd.merge(stocks_brent_df, gold_df, on='Date', how='inner')\n",
        "    return stocks_brent_gold_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a Spark DataFrame\n",
        "stocks_brent_gold_df_spark = spark.createDataFrame(stocks_brent_gold_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['symbol',\n",
              " 'name',\n",
              " 'trading_name ',\n",
              " 'Sector',\n",
              " 'Date',\n",
              " 'open',\n",
              " 'high',\n",
              " 'low',\n",
              " 'Stock_Price',\n",
              " 'change',\n",
              " 'perc_Change',\n",
              " 'volume_traded ',\n",
              " 'value_traded',\n",
              " 'no_trades ',\n",
              " 'Brent_Price',\n",
              " 'Gold_Price']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print columns of the DataFrame\n",
        "stocks_brent_gold_df_spark.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we cover the usage of various models to identify the best option to go with: Decision Tree, Regression Tree, SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.1 Model Selection: Decision Tree Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 10:34:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:34:52 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:34:53 WARN TaskSetManager: Stage 0 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:34:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:34:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# adding a new Price_Change column to the DataFrame, since the label should be categorical in Decision Trees and later indexed\n",
        "# this column will be used to determine if the stock price increased or decreased from the previous day\n",
        "\n",
        "\n",
        "stocks_brent_gold_df_spark = stocks_brent_gold_df_spark.withColumn(\"Price_Change\",\n",
        "                        F.when(F.col(\"Stock_Price\") > F.lag(\"Stock_Price\").over(Window.orderBy(\"Date\")), \"Up\")\n",
        "                        .otherwise(\"Down\"))\n",
        "\n",
        "# Convert Price_Change to a numerical label\n",
        "label_indexer = StringIndexer(inputCol=\"Price_Change\", outputCol=\"label\")\n",
        "stocks_brent_gold_df_spark = label_indexer.fit(stocks_brent_gold_df_spark).transform(stocks_brent_gold_df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create StringIndexers for categorical columns\n",
        "name_indexer = StringIndexer(inputCol=\"name\", outputCol=\"nameIndex\")\n",
        "sector_indexer = StringIndexer(inputCol=\"Sector\", outputCol=\"SectorIndex\")\n",
        "symbol_indexer = StringIndexer(inputCol=\"symbol\", outputCol=\"symbolIndex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a VectorAssembler for the features\n",
        "assembler = VectorAssembler(inputCols=[\"Stock_Price\", \"Brent_Price\", \"Gold_Price\"], outputCol=\"features\")\n",
        "stocks_brent_gold_df_spark = assembler.transform(stocks_brent_gold_df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train-test split\n",
        "splits = stocks_brent_gold_df_spark.randomSplit([0.8, 0.2])\n",
        "train = splits[0]\n",
        "test = splits[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN TaskSetManager: Stage 3 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN TaskSetManager: Stage 4 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:38:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        }
      ],
      "source": [
        "# Define the DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
        "pipeline = Pipeline(stages=[name_indexer, sector_indexer, symbol_indexer, assembler, dt])\n",
        "model = dt.fit(train) # Train the model on the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions on the test data\n",
        "predictions = model.transform(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 10:39:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:39:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:39:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:39:13 WARN TaskSetManager: Stage 31 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:39:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:39:13 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7102054064079381\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 10:47:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:47:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:47:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:47:12 WARN TaskSetManager: Stage 71 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:47:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:47:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-------------------+-----------+-----------+----------+-----+----------+\n",
            "|                name|               Date|Stock_Price|Brent_Price|Gold_Price|label|prediction|\n",
            "+--------------------+-------------------+-----------+-----------+----------+-----+----------+\n",
            "|Saudi Basic Indus...|2018-01-03 00:00:00|      101.5|      67.85|    1404.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-01-10 00:00:00|     101.68|      69.79|    1408.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-01-11 00:00:00|     101.54|      70.36|    1411.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-01-16 00:00:00|     106.16|       69.4|    1428.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-01-17 00:00:00|     107.24|      69.19|    1431.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-02-12 00:00:00|     105.06|       62.2|    1420.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-02-15 00:00:00|     105.29|      62.86|    1451.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-02-26 00:00:00|     105.15|      67.96|    1433.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-03-07 00:00:00|     106.34|      65.09|    1427.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-04-12 00:00:00|     116.73|      71.44|    1441.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-04-16 00:00:00|     120.18|      72.05|    1450.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-04-18 00:00:00|     121.26|      73.73|    1453.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-06-25 00:00:00|      128.2|      72.82|    1359.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-06-28 00:00:00|      126.2|      76.26|    1340.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-07-09 00:00:00|      125.8|      77.08|    1348.8|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-02 00:00:00|      128.6|      72.95|    1296.2|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-06 00:00:00|      128.0|      72.51|    1293.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-13 00:00:00|      123.2|      70.62|    1274.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-14 00:00:00|      123.4|      70.77|    1275.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-28 00:00:00|      126.8|      75.91|    1286.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-08-29 00:00:00|      127.4|      76.07|    1283.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-04 00:00:00|      126.2|      77.51|    1270.2|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-05 00:00:00|      120.6|      76.68|    1272.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-06 00:00:00|      118.2|      75.67|    1275.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-10 00:00:00|      119.0|      76.77|    1271.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-11 00:00:00|      119.8|      78.22|    1274.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-12 00:00:00|      119.0|      80.02|    1283.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-09-27 00:00:00|      123.6|      81.54|    1259.8|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-10-10 00:00:00|      122.4|      83.82|    1267.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-10-17 00:00:00|      125.8|      79.91|    1301.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-05 00:00:00|      125.4|      72.68|    1306.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-07 00:00:00|      124.0|       70.1|    1302.8|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-13 00:00:00|      122.8|      65.45|    1274.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-15 00:00:00|      119.8|      65.61|    1287.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-26 00:00:00|      115.4|       59.7|    1295.8|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-11-27 00:00:00|      115.6|      59.58|    1286.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2018-12-11 00:00:00|      118.4|      59.73|    1312.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-01-08 00:00:00|      119.2|      56.91|    1351.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-02-04 00:00:00|      123.0|      62.26|    1378.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-02-18 00:00:00|      123.4|      66.41|   1329.55|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-02-21 00:00:00|      123.4|      66.91|    1387.2|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-03-12 00:00:00|      121.8|      65.33|    1356.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-03-14 00:00:00|      124.2|      66.18|    1352.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-03-18 00:00:00|      125.0|      66.65|    1359.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-03-21 00:00:00|      125.0|       68.3|    1364.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-01 00:00:00|      125.0|      69.08|    1342.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-02 00:00:00|      126.0|      69.68|    1343.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-08 00:00:00|      127.2|      71.12|    1351.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-09 00:00:00|      127.2|      71.02|    1358.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-10 00:00:00|      125.4|      71.63|    1363.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-15 00:00:00|      125.0|       70.9|    1342.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-22 00:00:00|      124.2|      70.71|    1328.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-04-30 00:00:00|      124.2|      72.19|    1335.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-05-14 00:00:00|      112.0|      72.53|    1343.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-05-16 00:00:00|      113.2|       74.7|    1333.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-05-21 00:00:00|      112.8|      72.94|    1320.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-05-29 00:00:00|      110.0|      70.64|    1326.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-06-17 00:00:00|      118.6|      62.56|    1387.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-06-24 00:00:00|      113.6|      65.16|    1457.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-07-30 00:00:00|      108.6|      62.55|    1477.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-08-07 00:00:00|      104.4|      55.03|    1551.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-08-08 00:00:00|      104.0|      56.29|    1542.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-08-22 00:00:00|      102.4|      59.81|    1541.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-08-29 00:00:00|       99.9|      60.59|    1569.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-09-10 00:00:00|       96.5|      64.67|    1534.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-09-11 00:00:00|       94.4|      63.02|    1538.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-10-09 00:00:00|       89.2|       59.7|    1552.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-10-30 00:00:00|       87.9|      60.22|    1539.4|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-10-31 00:00:00|       87.7|       59.3|    1556.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-11-05 00:00:00|       88.5|      62.72|    1528.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-11-20 00:00:00|       94.9|       63.8|    1518.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-11-26 00:00:00|       90.3|      64.82|    1504.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-12-02 00:00:00|       90.1|       63.2|    1507.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-12-05 00:00:00|       90.0|      65.67|    1521.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-12-09 00:00:00|       93.0|      66.44|    1505.6|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2019-12-10 00:00:00|       92.2|      66.57|    1508.9|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-02 00:00:00|       93.4|      67.05|    1572.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-06 00:00:00|       91.9|      70.25|    1613.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-13 00:00:00|       92.7|      64.14|    1593.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-21 00:00:00|       92.0|      63.66|    1601.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-22 00:00:00|       91.8|      62.11|    1599.7|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-01-23 00:00:00|       90.9|      61.26|    1608.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-09 00:00:00|       64.1|      35.33|    1691.5|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-16 00:00:00|       62.0|      28.04|    1500.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-17 00:00:00|       64.1|      28.04|    1541.1|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-23 00:00:00|       65.1|      25.06|    1588.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-25 00:00:00|       67.2|      25.62|    1639.0|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-03-26 00:00:00|       69.0|      23.55|    1668.3|  0.0|       0.0|\n",
            "|Saudi Basic Indus...|2020-04-16 00:00:00|       76.3|      18.69|    1742.7|  0.0|       0.0|\n",
            "+--------------------+-------------------+-----------+-----------+----------+-----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create a table for the predictions and the actual labels for SABIC for dates after 2018\n",
        "predictions.createOrReplaceTempView(\"predictions\")\n",
        "sabic_predictions = spark.sql(\"SELECT name, Date, Stock_Price, Brent_Price, Gold_Price, label, prediction FROM predictions WHERE name = 'Saudi Basic Industries Corp.' AND Date > '2018-01-01'\").show(150)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 10:48:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:48:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:48:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:48:14 WARN TaskSetManager: Stage 74 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 10:48:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 10:48:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 2000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot the predictions and the actual labels for SABIC for dates after 2018\n",
        "sabic_predictions_df = predictions.toPandas()\n",
        "sabic_predictions_df = sabic_predictions_df[sabic_predictions_df['name'] == 'Saudi Basic Industries Corp.']\n",
        "sabic_predictions_df = sabic_predictions_df[sabic_predictions_df['Date'] > '2018-01-01']\n",
        "plt.figure(figsize=(20, 10))\n",
        "plt.plot(sabic_predictions_df['Date'], sabic_predictions_df['Stock_Price'], label='Stock Price')\n",
        "plt.plot(sabic_predictions_df['Date'], sabic_predictions_df['Brent_Price'], label='Brent Price')\n",
        "plt.plot(sabic_predictions_df['Date'], sabic_predictions_df['Gold_Price'], label='Gold Price')\n",
        "plt.plot(sabic_predictions_df['Date'], sabic_predictions_df['label'], label='Actual Label')\n",
        "plt.plot(sabic_predictions_df['Date'], sabic_predictions_df['prediction'], label='Prediction')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.2 Model Selection: Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# before we try different models, we need to preprocess the data again\n",
        "stocks_brent_gold_df = preprocess_data(stocks_df, brent_df, gold_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 11:59:45 WARN TaskSetManager: Stage 0 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "# because we are going to use the DecisionTreeRegressor, we need to convert the categorical data to numerical data\n",
        "indexer = StringIndexer(inputCol=\"Sector\", outputCol=\"SectorIndex\")\n",
        "indexed = indexer.fit(stocks_brent_gold_df_spark).transform(stocks_brent_gold_df_spark)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+\n",
            "|symbol|                name|  trading_name |   Sector|               Date| open| high|  low|Stock_Price|change|perc_Change|volume_traded |  value_traded|no_trades |Brent_Price|Gold_Price|SectorIndex|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+\n",
            "|  2030|Saudi Arabia Refi...|          SARCO|   Energy|2020-03-05 00:00:00|35.55|35.85| 34.9|       34.9|  -0.4|      -1.13|      436609.0|  1.53990735E7|     804.0|      51.29|    1690.5|        1.0|\n",
            "|  2222|Saudi Arabian Oil...|   SAUDI ARAMCO|   Energy|2020-03-05 00:00:00|33.05| 33.2|32.95|       33.0| -0.05|      -0.15|     3969243.0|1.3103890695E8|    4576.0|      51.29|    1690.5|        1.0|\n",
            "|  2380|Rabigh Refining a...|   PETRO RABIGH|   Energy|2020-03-05 00:00:00|15.18|15.18| 14.8|       14.8| -0.26|      -1.73|     1328273.0|  1.98755901E7|    1370.0|      51.29|    1690.5|        1.0|\n",
            "|  4030|National Shipping...|          BAHRI|   Energy|2020-03-05 00:00:00|31.95| 32.0| 30.8|      31.15| -0.55|      -1.74|      597076.0| 1.875434865E7|     796.0|      51.29|    1690.5|        1.0|\n",
            "|  4200|Aldrees Petroleum...|        ALDREES|   Energy|2020-03-05 00:00:00| 60.3| 61.3| 60.1|       60.1|   0.1|       0.17|      350180.0|  2.12106028E7|     734.0|      51.29|    1690.5|        1.0|\n",
            "|  1201|Takween Advanced ...|        TAKWEEN|Materials|2020-03-05 00:00:00| 7.68| 7.68|  7.5|       7.52| -0.04|      -0.53|      183425.0|     1386959.3|     275.0|      51.29|    1690.5|        0.0|\n",
            "|  1202|Middle East Paper...|          MEPCO|Materials|2020-03-05 00:00:00|13.66| 13.8|13.54|      13.54| -0.02|      -0.15|      584937.0|    7975687.54|     570.0|      51.29|    1690.5|        0.0|\n",
            "|  1210|Basic Chemical In...|            BCI|Materials|2020-03-05 00:00:00| 24.1| 24.3|23.84|       23.9| -0.06|      -0.25|      116835.0|    2807511.36|     218.0|      51.29|    1690.5|        0.0|\n",
            "|  1211|Saudi Arabian Min...|         MAADEN|Materials|2020-03-05 00:00:00| 36.0|36.35| 34.8|       34.8|  -0.8|      -2.25|      711699.0|  2.53748644E7|    1197.0|      51.29|    1690.5|        0.0|\n",
            "|  1301|United Wire Facto...|          ASLAK|Materials|2020-03-05 00:00:00| 19.5|19.56| 19.0|      19.04| -0.32|      -1.65|       76580.0|    1471655.68|     116.0|      51.29|    1690.5|        0.0|\n",
            "|  1304|Al Yamamah Steel ...|ALYAMAMAH STEEL|Materials|2020-03-05 00:00:00|20.02|20.12|19.62|       19.8| -0.18|       -0.9|      134742.0|    2671081.78|     243.0|      51.29|    1690.5|        0.0|\n",
            "|  1320|Saudi Steel Pipe Co.|            SSP|Materials|2020-03-05 00:00:00| 18.9| 19.1|18.56|      18.62|  0.06|       0.32|      241445.0|    4560501.24|     504.0|      51.29|    1690.5|        0.0|\n",
            "|  2001|Methanol Chemical...|       CHEMANOL|Materials|2020-03-05 00:00:00| 7.55| 7.57|  7.4|        7.4|  -0.1|      -1.33|      639345.0|    4798391.69|     585.0|      51.29|    1690.5|        0.0|\n",
            "|  2002|National Petroche...|      PETROCHEM|Materials|2020-03-05 00:00:00| 22.6| 22.7|22.56|      22.56| -0.12|      -0.53|     1746535.0| 3.940379092E7|     333.0|      51.29|    1690.5|        0.0|\n",
            "|  2020|Saudi Arabian Fer...|          SAFCO|Materials|2020-03-05 00:00:00| 68.4| 69.1| 67.4|       67.5|  -0.8|      -1.17|      191727.0|  1.30549484E7|     634.0|      51.29|    1690.5|        0.0|\n",
            "|  2060|National Industri...|         TASNEE|Materials|2020-03-05 00:00:00|11.54|11.64| 11.4|      11.42| -0.08|       -0.7|     1076342.0| 1.241573444E7|     833.0|      51.29|    1690.5|        0.0|\n",
            "|  2090| National Gypsum Co.|            NGC|Materials|2020-03-05 00:00:00|14.74|14.92|14.54|      14.58| -0.08|      -0.55|      402940.0|    5932614.32|     547.0|      51.29|    1690.5|        0.0|\n",
            "|  2150|The National Comp...|         ZOUJAJ|Materials|2020-03-05 00:00:00|16.02| 16.1|15.82|      15.82|  -0.1|      -0.63|       58023.0|     926556.96|     125.0|      51.29|    1690.5|        0.0|\n",
            "|  2170|Alujain Holding C...|        ALUJAIN|Materials|2020-03-05 00:00:00| 6.45|  7.1| 6.45|       33.0|   0.0|        0.0|           0.0|           0.0|       0.0|      51.29|    1690.5|        0.0|\n",
            "|  2180|Filing and Packin...|          FIPCO|Materials|2020-03-05 00:00:00| 38.8|39.75| 37.9|       39.2|   0.8|       2.08|     1112098.0|  4.32869109E7|    1666.0|      51.29|    1690.5|        0.0|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 11:59:52 WARN TaskSetManager: Stage 4 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        }
      ],
      "source": [
        "# index the name of the stock\n",
        "indexer = StringIndexer(inputCol=\"name\", outputCol=\"StockIndex\")\n",
        "indexed = indexer.fit(indexed).transform(indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+\n",
            "|symbol|                name|  trading_name |   Sector|               Date| open| high|  low|Stock_Price|change|perc_Change|volume_traded |  value_traded|no_trades |Brent_Price|Gold_Price|SectorIndex|StockIndex|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+\n",
            "|  2030|Saudi Arabia Refi...|          SARCO|   Energy|2020-03-05 00:00:00|35.55|35.85| 34.9|       34.9|  -0.4|      -1.13|      436609.0|  1.53990735E7|     804.0|      51.29|    1690.5|        1.0|      18.0|\n",
            "|  2222|Saudi Arabian Oil...|   SAUDI ARAMCO|   Energy|2020-03-05 00:00:00|33.05| 33.2|32.95|       33.0| -0.05|      -0.15|     3969243.0|1.3103890695E8|    4576.0|      51.29|    1690.5|        1.0|      46.0|\n",
            "|  2380|Rabigh Refining a...|   PETRO RABIGH|   Energy|2020-03-05 00:00:00|15.18|15.18| 14.8|       14.8| -0.26|      -1.73|     1328273.0|  1.98755901E7|    1370.0|      51.29|    1690.5|        1.0|      28.0|\n",
            "|  4030|National Shipping...|          BAHRI|   Energy|2020-03-05 00:00:00|31.95| 32.0| 30.8|      31.15| -0.55|      -1.74|      597076.0| 1.875434865E7|     796.0|      51.29|    1690.5|        1.0|       1.0|\n",
            "|  4200|Aldrees Petroleum...|        ALDREES|   Energy|2020-03-05 00:00:00| 60.3| 61.3| 60.1|       60.1|   0.1|       0.17|      350180.0|  2.12106028E7|     734.0|      51.29|    1690.5|        1.0|      23.0|\n",
            "|  1201|Takween Advanced ...|        TAKWEEN|Materials|2020-03-05 00:00:00| 7.68| 7.68|  7.5|       7.52| -0.04|      -0.53|      183425.0|     1386959.3|     275.0|      51.29|    1690.5|        0.0|      37.0|\n",
            "|  1202|Middle East Paper...|          MEPCO|Materials|2020-03-05 00:00:00|13.66| 13.8|13.54|      13.54| -0.02|      -0.15|      584937.0|    7975687.54|     570.0|      51.29|    1690.5|        0.0|      42.0|\n",
            "|  1210|Basic Chemical In...|            BCI|Materials|2020-03-05 00:00:00| 24.1| 24.3|23.84|       23.9| -0.06|      -0.25|      116835.0|    2807511.36|     218.0|      51.29|    1690.5|        0.0|      29.0|\n",
            "|  1211|Saudi Arabian Min...|         MAADEN|Materials|2020-03-05 00:00:00| 36.0|36.35| 34.8|       34.8|  -0.8|      -2.25|      711699.0|  2.53748644E7|    1197.0|      51.29|    1690.5|        0.0|      30.0|\n",
            "|  1301|United Wire Facto...|          ASLAK|Materials|2020-03-05 00:00:00| 19.5|19.56| 19.0|      19.04| -0.32|      -1.65|       76580.0|    1471655.68|     116.0|      51.29|    1690.5|        0.0|      35.0|\n",
            "|  1304|Al Yamamah Steel ...|ALYAMAMAH STEEL|Materials|2020-03-05 00:00:00|20.02|20.12|19.62|       19.8| -0.18|       -0.9|      134742.0|    2671081.78|     243.0|      51.29|    1690.5|        0.0|      43.0|\n",
            "|  1320|Saudi Steel Pipe Co.|            SSP|Materials|2020-03-05 00:00:00| 18.9| 19.1|18.56|      18.62|  0.06|       0.32|      241445.0|    4560501.24|     504.0|      51.29|    1690.5|        0.0|      32.0|\n",
            "|  2001|Methanol Chemical...|       CHEMANOL|Materials|2020-03-05 00:00:00| 7.55| 7.57|  7.4|        7.4|  -0.1|      -1.33|      639345.0|    4798391.69|     585.0|      51.29|    1690.5|        0.0|      31.0|\n",
            "|  2002|National Petroche...|      PETROCHEM|Materials|2020-03-05 00:00:00| 22.6| 22.7|22.56|      22.56| -0.12|      -0.53|     1746535.0| 3.940379092E7|     333.0|      51.29|    1690.5|        0.0|      33.0|\n",
            "|  2020|Saudi Arabian Fer...|          SAFCO|Materials|2020-03-05 00:00:00| 68.4| 69.1| 67.4|       67.5|  -0.8|      -1.17|      191727.0|  1.30549484E7|     634.0|      51.29|    1690.5|        0.0|       2.0|\n",
            "|  2060|National Industri...|         TASNEE|Materials|2020-03-05 00:00:00|11.54|11.64| 11.4|      11.42| -0.08|       -0.7|     1076342.0| 1.241573444E7|     833.0|      51.29|    1690.5|        0.0|       0.0|\n",
            "|  2090| National Gypsum Co.|            NGC|Materials|2020-03-05 00:00:00|14.74|14.92|14.54|      14.58| -0.08|      -0.55|      402940.0|    5932614.32|     547.0|      51.29|    1690.5|        0.0|      14.0|\n",
            "|  2150|The National Comp...|         ZOUJAJ|Materials|2020-03-05 00:00:00|16.02| 16.1|15.82|      15.82|  -0.1|      -0.63|       58023.0|     926556.96|     125.0|      51.29|    1690.5|        0.0|      17.0|\n",
            "|  2170|Alujain Holding C...|        ALUJAIN|Materials|2020-03-05 00:00:00| 6.45|  7.1| 6.45|       33.0|   0.0|        0.0|           0.0|           0.0|       0.0|      51.29|    1690.5|        0.0|       7.0|\n",
            "|  2180|Filing and Packin...|          FIPCO|Materials|2020-03-05 00:00:00| 38.8|39.75| 37.9|       39.2|   0.8|       2.08|     1112098.0|  4.32869109E7|    1666.0|      51.29|    1690.5|        0.0|      19.0|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# verify the indexing\n",
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a feature vector by combining the features that we are going to use\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"Stock_Price\", \"volume_traded \", \"value_traded\", \"no_trades \", \"Brent_Price\", \"Gold_Price\"], outputCol=\"features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a VectorAssembler for the features\n",
        "assembledData = vectorAssembler.transform(indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "stocks_brent_gold_df_spark = assembledData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+----------------------------------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+---------------------------------------------------+\n",
            "|symbol|name                                          |trading_name   |Sector   |Date               |open |high |low  |Stock_Price|change|perc_Change|volume_traded |value_traded  |no_trades |Brent_Price|Gold_Price|SectorIndex|StockIndex|features                                           |\n",
            "+------+----------------------------------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+---------------------------------------------------+\n",
            "|2030  |Saudi Arabia Refineries Co.                   |SARCO          |Energy   |2020-03-05 00:00:00|35.55|35.85|34.9 |34.9       |-0.4  |-1.13      |436609.0      |1.53990735E7  |804.0     |51.29      |1690.5    |1.0        |18.0      |[34.9,436609.0,1.53990735E7,804.0,51.29,1690.5]    |\n",
            "|2222  |Saudi Arabian Oil Co.                         |SAUDI ARAMCO   |Energy   |2020-03-05 00:00:00|33.05|33.2 |32.95|33.0       |-0.05 |-0.15      |3969243.0     |1.3103890695E8|4576.0    |51.29      |1690.5    |1.0        |46.0      |[33.0,3969243.0,1.3103890695E8,4576.0,51.29,1690.5]|\n",
            "|2380  |Rabigh Refining and Petrochemical Co.         |PETRO RABIGH   |Energy   |2020-03-05 00:00:00|15.18|15.18|14.8 |14.8       |-0.26 |-1.73      |1328273.0     |1.98755901E7  |1370.0    |51.29      |1690.5    |1.0        |28.0      |[14.8,1328273.0,1.98755901E7,1370.0,51.29,1690.5]  |\n",
            "|4030  |National Shipping Company of Saudi Arabia     |BAHRI          |Energy   |2020-03-05 00:00:00|31.95|32.0 |30.8 |31.15      |-0.55 |-1.74      |597076.0      |1.875434865E7 |796.0     |51.29      |1690.5    |1.0        |1.0       |[31.15,597076.0,1.875434865E7,796.0,51.29,1690.5]  |\n",
            "|4200  |Aldrees Petroleum and Transport Services Co.  |ALDREES        |Energy   |2020-03-05 00:00:00|60.3 |61.3 |60.1 |60.1       |0.1   |0.17       |350180.0      |2.12106028E7  |734.0     |51.29      |1690.5    |1.0        |23.0      |[60.1,350180.0,2.12106028E7,734.0,51.29,1690.5]    |\n",
            "|1201  |Takween Advanced Industries Co.               |TAKWEEN        |Materials|2020-03-05 00:00:00|7.68 |7.68 |7.5  |7.52       |-0.04 |-0.53      |183425.0      |1386959.3     |275.0     |51.29      |1690.5    |0.0        |37.0      |[7.52,183425.0,1386959.3,275.0,51.29,1690.5]       |\n",
            "|1202  |Middle East Paper Co.                         |MEPCO          |Materials|2020-03-05 00:00:00|13.66|13.8 |13.54|13.54      |-0.02 |-0.15      |584937.0      |7975687.54    |570.0     |51.29      |1690.5    |0.0        |42.0      |[13.54,584937.0,7975687.54,570.0,51.29,1690.5]     |\n",
            "|1210  |Basic Chemical Industries Co.                 |BCI            |Materials|2020-03-05 00:00:00|24.1 |24.3 |23.84|23.9       |-0.06 |-0.25      |116835.0      |2807511.36    |218.0     |51.29      |1690.5    |0.0        |29.0      |[23.9,116835.0,2807511.36,218.0,51.29,1690.5]      |\n",
            "|1211  |Saudi Arabian Mining Co.                      |MAADEN         |Materials|2020-03-05 00:00:00|36.0 |36.35|34.8 |34.8       |-0.8  |-2.25      |711699.0      |2.53748644E7  |1197.0    |51.29      |1690.5    |0.0        |30.0      |[34.8,711699.0,2.53748644E7,1197.0,51.29,1690.5]   |\n",
            "|1301  |United Wire Factories Co.                     |ASLAK          |Materials|2020-03-05 00:00:00|19.5 |19.56|19.0 |19.04      |-0.32 |-1.65      |76580.0       |1471655.68    |116.0     |51.29      |1690.5    |0.0        |35.0      |[19.04,76580.0,1471655.68,116.0,51.29,1690.5]      |\n",
            "|1304  |Al Yamamah Steel Industries Co.               |ALYAMAMAH STEEL|Materials|2020-03-05 00:00:00|20.02|20.12|19.62|19.8       |-0.18 |-0.9       |134742.0      |2671081.78    |243.0     |51.29      |1690.5    |0.0        |43.0      |[19.8,134742.0,2671081.78,243.0,51.29,1690.5]      |\n",
            "|1320  |Saudi Steel Pipe Co.                          |SSP            |Materials|2020-03-05 00:00:00|18.9 |19.1 |18.56|18.62      |0.06  |0.32       |241445.0      |4560501.24    |504.0     |51.29      |1690.5    |0.0        |32.0      |[18.62,241445.0,4560501.24,504.0,51.29,1690.5]     |\n",
            "|2001  |Methanol Chemicals Co.                        |CHEMANOL       |Materials|2020-03-05 00:00:00|7.55 |7.57 |7.4  |7.4        |-0.1  |-1.33      |639345.0      |4798391.69    |585.0     |51.29      |1690.5    |0.0        |31.0      |[7.4,639345.0,4798391.69,585.0,51.29,1690.5]       |\n",
            "|2002  |National Petrochemical Co.                    |PETROCHEM      |Materials|2020-03-05 00:00:00|22.6 |22.7 |22.56|22.56      |-0.12 |-0.53      |1746535.0     |3.940379092E7 |333.0     |51.29      |1690.5    |0.0        |33.0      |[22.56,1746535.0,3.940379092E7,333.0,51.29,1690.5] |\n",
            "|2020  |Saudi Arabian Fertilizer Co.                  |SAFCO          |Materials|2020-03-05 00:00:00|68.4 |69.1 |67.4 |67.5       |-0.8  |-1.17      |191727.0      |1.30549484E7  |634.0     |51.29      |1690.5    |0.0        |2.0       |[67.5,191727.0,1.30549484E7,634.0,51.29,1690.5]    |\n",
            "|2060  |National Industrialization Co.                |TASNEE         |Materials|2020-03-05 00:00:00|11.54|11.64|11.4 |11.42      |-0.08 |-0.7       |1076342.0     |1.241573444E7 |833.0     |51.29      |1690.5    |0.0        |0.0       |[11.42,1076342.0,1.241573444E7,833.0,51.29,1690.5] |\n",
            "|2090  |National Gypsum Co.                           |NGC            |Materials|2020-03-05 00:00:00|14.74|14.92|14.54|14.58      |-0.08 |-0.55      |402940.0      |5932614.32    |547.0     |51.29      |1690.5    |0.0        |14.0      |[14.58,402940.0,5932614.32,547.0,51.29,1690.5]     |\n",
            "|2150  |The National Company for Glass Industries     |ZOUJAJ         |Materials|2020-03-05 00:00:00|16.02|16.1 |15.82|15.82      |-0.1  |-0.63      |58023.0       |926556.96     |125.0     |51.29      |1690.5    |0.0        |17.0      |[15.82,58023.0,926556.96,125.0,51.29,1690.5]       |\n",
            "|2170  |Alujain Holding Corp.                         |ALUJAIN        |Materials|2020-03-05 00:00:00|6.45 |7.1  |6.45 |33.0       |0.0   |0.0        |0.0           |0.0           |0.0       |51.29      |1690.5    |0.0        |7.0       |[33.0,0.0,0.0,0.0,51.29,1690.5]                    |\n",
            "|2180  |Filing and Packing Materials Manufacturing Co.|FIPCO          |Materials|2020-03-05 00:00:00|38.8 |39.75|37.9 |39.2       |0.8   |2.08       |1112098.0     |4.32869109E7  |1666.0    |51.29      |1690.5    |0.0        |19.0      |[39.2,1112098.0,4.32869109E7,1666.0,51.29,1690.5]  |\n",
            "+------+----------------------------------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+----------+---------------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show the datframe to verify features column\n",
        "stocks_brent_gold_df_spark.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cross validation on spark doesn't respect the time in time-series; test:train is better on spark\n",
        "# equivilant of sklearn.model_selection.TimeSeriesSplit\n",
        "# else: hold-out method\n",
        "# later on, use other models: SVM (super vector machine) / MLP / etc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the data into training and testing\n",
        "# 80% of the data is used for training and 20% for testing\n",
        "(trainingData, testData) = stocks_brent_gold_df_spark.randomSplit([0.8, 0.2])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"StockIndex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define the evaluator\n",
        "# note the metricName parameter is rmse, which stands for Root Mean Squared Error. This is the default metric for regression problems.\n",
        "# Other metrics include r2 (R squared) and mae (Mean Absolute Error)\n",
        "# see the documentation for more details: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"StockIndex\", predictionCol=\"prediction\", metricName=\"rmse\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the grid of hyperparameters\n",
        "# We will use a ParamGridBuilder to construct a grid of parameters to search over.\n",
        "# With 3 values for dt.maxDepth and 5 values for dt.maxBins, this grid will have 3 x 5 = 15 parameter settings for CrossValidator to choose from.\n",
        "# see the documentation for more details: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder\n",
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(dt.maxDepth, [2, 3, 4, 5, 6, 7, 8, 9, 10]) \\\n",
        "    .addGrid(dt.maxBins, [10, 20, 40, 80, 100]) \\\n",
        "    .build()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the CrossValidator\n",
        "# We will use a CrossValidator to select the best model.\n",
        "# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
        "# see the documentation for more details: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator\n",
        "cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 15:42:10 WARN TaskSetManager: Stage 11 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:10 WARN TaskSetManager: Stage 13 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:10 WARN TaskSetManager: Stage 14 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:10 WARN TaskSetManager: Stage 16 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 18 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 20 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 21 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 24 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 25 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:11 WARN TaskSetManager: Stage 27 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 29 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 31 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 34 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 35 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 37 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 39 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 41 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 44 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 45 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 47 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 49 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:12 WARN TaskSetManager: Stage 51 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 54 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 55 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 57 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 59 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 61 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 64 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 65 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 67 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 69 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 71 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 73 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 76 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 77 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:13 WARN TaskSetManager: Stage 79 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 81 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 83 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 85 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 88 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 89 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 91 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 93 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 95 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 97 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 100 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 101 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 103 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 105 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 107 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 109 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 112 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 113 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:14 WARN TaskSetManager: Stage 115 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 117 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 119 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 121 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 124 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 125 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 127 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 129 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 131 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 133 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 135 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 138 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 139 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 141 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 143 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 145 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 147 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 149 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 152 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 153 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:15 WARN TaskSetManager: Stage 155 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 157 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 159 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 161 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 163 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 166 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 167 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 169 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 171 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 173 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 175 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 177 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 180 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 181 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 183 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 185 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 187 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 189 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 191 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 194 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 195 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:16 WARN TaskSetManager: Stage 197 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 199 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 201 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 203 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 205 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 207 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 210 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 211 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 213 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 215 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 217 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 219 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 221 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 223 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 226 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 227 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 229 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 231 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 233 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 235 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 237 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 239 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 242 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 243 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:17 WARN TaskSetManager: Stage 245 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 247 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 249 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 251 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 253 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 255 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 258 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 259 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 261 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 263 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 265 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 267 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 269 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 271 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 274 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 275 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 277 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 279 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 281 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 283 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 285 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 287 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 289 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 292 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:18 WARN TaskSetManager: Stage 293 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 295 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 297 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 299 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 301 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 303 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 305 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 307 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 310 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 311 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 313 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 315 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 317 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 319 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 321 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 323 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 325 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 328 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 329 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 331 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 333 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 335 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 337 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 339 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:19 WARN TaskSetManager: Stage 341 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 343 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 346 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 347 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 349 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 351 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 353 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 355 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 357 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 359 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 361 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 364 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 365 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 367 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 369 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 371 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 373 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 375 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 377 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 379 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 381 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 384 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 385 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 387 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:20 WARN TaskSetManager: Stage 389 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 391 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 393 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 395 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 397 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 399 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 401 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 404 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 405 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 407 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 409 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 411 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 413 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 415 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 417 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 419 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 421 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 424 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 425 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 427 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 429 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 431 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 433 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 435 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 437 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 439 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:21 WARN TaskSetManager: Stage 441 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 444 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 445 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 447 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 449 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 451 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 453 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 455 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 457 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 459 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 461 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 464 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 465 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 467 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 469 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 471 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 473 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 475 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 477 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 479 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:22 WARN TaskSetManager: Stage 481 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 483 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 486 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 487 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 489 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 491 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 493 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 495 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 497 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 499 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 501 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 503 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 505 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 508 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 509 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 511 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 513 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 515 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 517 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 519 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 521 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 523 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 525 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 527 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 530 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:23 WARN TaskSetManager: Stage 531 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 533 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 535 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 537 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 539 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 541 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 543 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 545 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 547 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 549 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 552 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 553 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 555 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 557 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 559 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 561 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 563 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 565 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 567 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 569 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 571 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 574 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 575 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:24 WARN TaskSetManager: Stage 577 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 579 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 581 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 583 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 585 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 587 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 589 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 591 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 593 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 595 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 598 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 599 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 601 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 603 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 605 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 607 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 609 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 611 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 613 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 615 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 617 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 619 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 622 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 623 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 625 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:25 WARN TaskSetManager: Stage 627 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 629 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 631 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 633 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 635 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 637 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 639 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 641 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 643 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 646 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 647 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 649 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 651 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 653 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 655 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 657 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 659 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 661 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 663 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 665 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 667 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 670 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 671 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:26 WARN TaskSetManager: Stage 673 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 675 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 677 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 679 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 681 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 683 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 685 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 687 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 689 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 691 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 694 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 695 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 697 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 699 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 701 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 703 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 705 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 707 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 709 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 711 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 713 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 715 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 717 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 720 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 721 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:27 WARN TaskSetManager: Stage 723 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 725 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 727 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 729 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 731 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 733 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 735 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 737 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 739 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 741 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 743 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 746 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 747 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 749 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 751 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 753 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 755 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 757 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 759 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 761 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 763 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 765 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 767 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 769 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:28 WARN TaskSetManager: Stage 772 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 773 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 775 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 777 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 779 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 781 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 783 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 785 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 787 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 789 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 791 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 793 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 795 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 798 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 799 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 801 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 803 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 805 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 807 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:29 WARN TaskSetManager: Stage 809 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 811 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 813 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 815 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 817 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 819 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 821 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 823 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 825 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 826 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 828 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 830 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:30 WARN TaskSetManager: Stage 832 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 833 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 836 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 837 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 839 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 841 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 843 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 846 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 847 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 849 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 851 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 853 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 856 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 857 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 859 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 861 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 863 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 866 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:31 WARN TaskSetManager: Stage 867 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 869 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 871 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 873 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 876 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 877 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 879 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 881 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 883 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 885 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 888 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 889 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 891 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 893 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 895 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 897 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 900 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 901 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 903 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 905 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:32 WARN TaskSetManager: Stage 907 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 909 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 912 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 913 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 915 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 917 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 919 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 921 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 924 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 925 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 927 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 929 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 931 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 933 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 936 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 937 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 939 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 941 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 943 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 945 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 947 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 950 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 951 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:33 WARN TaskSetManager: Stage 953 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 955 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 957 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 959 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 961 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 964 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 965 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 967 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 969 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 971 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 973 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 975 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 978 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 979 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 981 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 983 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 985 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 987 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 989 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 992 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 993 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:34 WARN TaskSetManager: Stage 995 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 997 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 999 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1001 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1003 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1006 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1007 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1009 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1011 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1013 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1015 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1017 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1019 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1022 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1023 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1025 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1027 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1029 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1031 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1033 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1035 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1038 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1039 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:35 WARN TaskSetManager: Stage 1041 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1043 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1045 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1047 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1049 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1051 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1054 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1055 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1057 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1059 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1061 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1063 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1065 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1067 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1070 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1071 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1073 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1075 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1077 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1079 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1081 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1083 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1086 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1087 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:36 WARN TaskSetManager: Stage 1089 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1091 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1093 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1095 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1097 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1099 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1101 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1104 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1105 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1107 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1109 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1111 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1113 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1115 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1117 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1119 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1122 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1123 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1125 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1127 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1129 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1131 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1133 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1135 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1137 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1140 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:37 WARN TaskSetManager: Stage 1141 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1143 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1145 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1147 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1149 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1151 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1153 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1155 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1158 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1159 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1161 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1163 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1165 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1167 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1169 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1171 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1173 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1176 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1177 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1179 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1181 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1183 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1185 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1187 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1189 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1191 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:38 WARN TaskSetManager: Stage 1193 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1196 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1197 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1199 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1201 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1203 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1205 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1207 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1209 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1211 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1213 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1216 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1217 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1219 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1221 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1223 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1225 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1227 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1229 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1231 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1233 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1236 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1237 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1239 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1241 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1243 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:39 WARN TaskSetManager: Stage 1245 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1247 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1249 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1251 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1253 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1256 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1257 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1259 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1261 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1263 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1265 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1267 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1269 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1271 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1273 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1276 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1277 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1279 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1281 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1283 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1285 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1287 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1289 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1291 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1293 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1295 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1298 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:40 WARN TaskSetManager: Stage 1299 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1301 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1303 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1305 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1307 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1309 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1311 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1313 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1315 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1317 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1320 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1321 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1323 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1325 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1327 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1329 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1331 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1333 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1335 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1337 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1339 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1342 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1343 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1345 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1347 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1349 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:41 WARN TaskSetManager: Stage 1351 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1353 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1355 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1357 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1359 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1361 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1364 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1365 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1367 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1369 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1371 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1373 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1375 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1377 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1379 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1381 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1383 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1386 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1387 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1389 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1391 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1393 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:42 WARN TaskSetManager: Stage 1395 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1397 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1399 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1401 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1403 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1405 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1407 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1410 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1411 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1413 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1415 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1417 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1419 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1421 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1423 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1425 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1427 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1429 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1431 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1434 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1435 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1437 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1439 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1441 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1443 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1445 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1447 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1449 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:43 WARN TaskSetManager: Stage 1451 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1453 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1455 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1458 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1459 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1461 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1463 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1465 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1467 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1469 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1471 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1473 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1475 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1477 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1479 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1482 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1483 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1485 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1487 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1489 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1491 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1493 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1495 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1497 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:44 WARN TaskSetManager: Stage 1499 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1501 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1503 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1506 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1507 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1509 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1511 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1513 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1515 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1517 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1519 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1521 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1523 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1525 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1527 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1529 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1532 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1533 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1535 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1537 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1539 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1541 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1543 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1545 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1547 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1549 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:45 WARN TaskSetManager: Stage 1551 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1553 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1555 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1558 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1559 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1561 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1563 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1565 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1567 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1569 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1571 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1573 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1575 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1577 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1579 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1581 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1584 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1585 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1587 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1589 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1591 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1593 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1595 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1597 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1599 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:46 WARN TaskSetManager: Stage 1601 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1603 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1605 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1607 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1610 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1611 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1613 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1615 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1617 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1619 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1621 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1623 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1625 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1627 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1629 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1631 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1633 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:47 WARN TaskSetManager: Stage 1635 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1637 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1638 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1640 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1642 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1644 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1645 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1648 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1649 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1651 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1653 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1655 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1658 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1659 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:48 WARN TaskSetManager: Stage 1661 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1663 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1665 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1668 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1669 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1671 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1673 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1675 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1678 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1679 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1681 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1683 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1685 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1688 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1689 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1691 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1693 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1695 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1697 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1700 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:49 WARN TaskSetManager: Stage 1701 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1703 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1705 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1707 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1709 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1712 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1713 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1715 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1717 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1719 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1721 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1724 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1725 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1727 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1729 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1731 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1733 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1736 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1737 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1739 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1741 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:50 WARN TaskSetManager: Stage 1743 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1745 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1748 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1749 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1751 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1753 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1755 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1757 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1759 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1762 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1763 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1765 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1767 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1769 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1771 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1773 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1776 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1777 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1779 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1781 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1783 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1785 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1787 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1790 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:51 WARN TaskSetManager: Stage 1791 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1793 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1795 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1797 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1799 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1801 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1804 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1805 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1807 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1809 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1811 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1813 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1815 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1818 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1819 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1821 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1823 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1825 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1827 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1829 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1831 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1834 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1835 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:52 WARN TaskSetManager: Stage 1837 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1839 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1841 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1843 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1845 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1847 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1850 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1851 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1853 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1855 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1857 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1859 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1861 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1863 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1866 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1867 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1869 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1871 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1873 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1875 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:53 WARN TaskSetManager: Stage 1877 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1879 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1882 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1883 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1885 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1887 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1889 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1891 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1893 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1895 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1898 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1899 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1901 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1903 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1905 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1907 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1909 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1911 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1913 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1916 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1917 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1919 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1921 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1923 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1925 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1927 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:54 WARN TaskSetManager: Stage 1929 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1931 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1934 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1935 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1937 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1939 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1941 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1943 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1945 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1947 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1949 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1952 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1953 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1955 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1957 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1959 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1961 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1963 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1965 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1967 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1970 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1971 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1973 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1975 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1977 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1979 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:55 WARN TaskSetManager: Stage 1981 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1983 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1985 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1988 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1989 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1991 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1993 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1995 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1997 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 1999 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2001 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2003 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2005 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2008 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2009 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2011 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2013 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2015 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2017 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2019 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2021 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2023 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2025 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2028 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2029 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:56 WARN TaskSetManager: Stage 2031 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2033 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2035 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2037 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2039 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2041 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2043 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2045 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2048 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2049 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2051 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2053 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2055 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2057 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2059 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2061 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2063 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2065 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2068 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2069 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2071 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2073 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2075 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2077 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2079 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2081 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:57 WARN TaskSetManager: Stage 2083 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2085 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2088 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2089 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2091 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2093 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2095 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2097 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2099 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2101 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2103 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2105 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2107 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2110 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2111 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2113 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2115 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2117 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2119 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2121 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2123 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2125 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2127 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2129 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2132 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2133 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:58 WARN TaskSetManager: Stage 2135 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2137 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2139 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2141 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2143 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2145 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2147 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2149 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2151 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2154 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2155 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2157 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2159 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2161 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2163 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2165 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2167 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2169 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2171 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2173 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2176 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2177 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2179 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2181 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2183 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2185 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:42:59 WARN TaskSetManager: Stage 2187 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2189 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2191 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2193 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2195 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2198 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2199 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2201 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2203 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2205 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2207 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2209 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2211 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2213 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2215 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2217 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2219 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2222 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2223 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2225 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2227 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2229 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2231 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2233 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2235 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2237 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2239 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:00 WARN TaskSetManager: Stage 2241 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2243 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2246 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2247 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2249 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2251 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2253 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2255 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2257 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2259 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2261 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2263 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2265 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2267 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2270 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2271 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2273 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2275 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2277 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2279 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2281 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2283 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:01 WARN TaskSetManager: Stage 2285 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2287 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2289 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2291 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2294 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2295 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2297 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2299 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2301 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2303 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2305 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2307 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2309 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2311 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2313 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2315 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2318 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2319 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2321 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2323 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2325 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2327 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2329 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2331 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:02 WARN TaskSetManager: Stage 2333 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2335 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2337 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2339 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2341 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2344 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2345 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2347 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2349 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2351 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2353 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2355 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2357 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2359 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2361 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2363 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2365 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2367 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2370 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2371 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2373 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2375 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2377 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2379 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2381 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:03 WARN TaskSetManager: Stage 2383 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2385 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2387 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2389 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2391 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2393 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2396 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2397 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2399 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2401 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2403 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2405 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2407 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2409 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2411 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2413 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2415 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2417 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2419 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2422 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2423 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2425 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:04 WARN TaskSetManager: Stage 2427 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2429 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2431 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2433 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2435 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2437 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2439 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2441 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2443 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2445 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2447 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2449 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2450 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2452 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:05 WARN TaskSetManager: Stage 2454 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2456 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2457 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2460 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2461 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2463 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2465 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2467 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2470 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2471 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2473 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2475 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2477 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2480 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2481 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2483 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2485 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:06 WARN TaskSetManager: Stage 2487 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2490 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2491 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2493 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2495 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2497 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2500 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2501 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2503 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2505 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2507 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2509 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2512 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2513 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2515 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2517 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2519 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2521 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2524 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2525 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2527 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:07 WARN TaskSetManager: Stage 2529 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2531 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2533 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2536 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2537 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2539 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2541 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2543 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2545 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2548 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2549 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2551 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2553 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2555 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2557 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2560 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2561 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2563 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2565 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2567 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2569 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2571 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2574 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:08 WARN TaskSetManager: Stage 2575 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2577 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2579 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2581 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2583 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2585 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2588 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2589 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2591 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2593 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2595 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2597 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2599 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2602 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2603 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2605 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2607 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2609 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2611 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2613 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2616 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2617 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:09 WARN TaskSetManager: Stage 2619 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2621 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2623 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2625 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2627 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2630 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2631 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2633 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2635 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2637 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2639 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2641 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2643 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2646 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2647 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2649 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2651 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2653 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2655 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2657 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2659 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2662 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2663 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:10 WARN TaskSetManager: Stage 2665 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2667 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2669 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2671 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2673 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2675 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2678 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2679 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2681 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2683 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2685 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2687 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2689 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2691 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2694 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2695 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2697 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2699 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2701 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2703 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2705 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2707 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2710 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2711 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:11 WARN TaskSetManager: Stage 2713 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2715 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2717 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2719 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2721 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2723 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2725 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2728 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2729 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2731 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2733 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2735 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2737 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2739 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2741 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2743 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2746 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2747 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2749 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2751 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2753 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2755 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2757 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2759 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2761 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:12 WARN TaskSetManager: Stage 2764 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2765 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2767 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2769 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2771 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2773 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2775 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2777 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2779 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2782 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2783 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2785 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2787 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2789 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2791 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2793 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2795 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2797 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2800 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2801 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2803 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2805 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2807 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2809 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2811 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2813 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:13 WARN TaskSetManager: Stage 2815 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2817 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2820 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2821 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2823 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2825 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2827 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2829 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2831 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2833 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2835 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2837 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2840 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2841 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2843 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2845 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2847 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2849 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2851 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2853 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2855 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2857 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2860 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2861 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:14 WARN TaskSetManager: Stage 2863 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2865 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2867 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2869 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2871 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2873 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2875 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2877 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2880 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2881 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2883 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2885 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2887 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2889 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2891 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2893 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2895 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2897 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2900 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2901 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2903 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2905 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2907 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2909 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2911 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2913 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2915 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2917 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:15 WARN TaskSetManager: Stage 2919 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2922 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2923 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2925 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2927 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2929 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2931 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2933 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2935 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2937 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2939 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2941 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2944 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2945 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2947 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2949 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2951 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2953 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2955 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2957 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2959 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2961 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2963 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2966 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2967 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:16 WARN TaskSetManager: Stage 2969 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2971 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2973 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2975 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2977 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2979 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2981 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2983 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2985 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2988 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2989 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2991 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2993 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2995 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2997 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 2999 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3001 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3003 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3005 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3007 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3010 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3011 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3013 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3015 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3017 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3019 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3021 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3023 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3025 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:17 WARN TaskSetManager: Stage 3027 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3029 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3031 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3034 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3035 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3037 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3039 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3041 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3043 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3045 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3047 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3049 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3051 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3053 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3055 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3058 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3059 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3061 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3063 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3065 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3067 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3069 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3071 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3073 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3075 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:18 WARN TaskSetManager: Stage 3077 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3079 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3082 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3083 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3085 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3087 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3089 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3091 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3093 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3095 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3097 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3099 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3101 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3103 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3106 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3107 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3109 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3111 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3113 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3115 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3117 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3119 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:19 WARN TaskSetManager: Stage 3121 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3123 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3125 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3127 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3130 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3131 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3133 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3135 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3137 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3139 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3141 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3143 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3145 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3147 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3149 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3151 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3153 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3156 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3157 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3159 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3161 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3163 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3165 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3167 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3169 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3171 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:20 WARN TaskSetManager: Stage 3173 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3175 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3177 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3179 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3182 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3183 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3185 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3187 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3189 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3191 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3193 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3195 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3197 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3199 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3201 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3203 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3205 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3208 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3209 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3211 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3213 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3215 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3217 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:21 WARN TaskSetManager: Stage 3219 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3221 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3223 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3225 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3227 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3229 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3231 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3234 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3235 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3237 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3239 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3241 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3243 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3245 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3247 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3249 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3251 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3253 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3255 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:22 WARN TaskSetManager: Stage 3257 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3259 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3261 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3262 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3264 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3266 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3268 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3269 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3272 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3273 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3275 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3277 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3279 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3282 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:23 WARN TaskSetManager: Stage 3283 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3285 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3287 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3289 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3292 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3293 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3295 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3297 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3299 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3302 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3303 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3305 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3307 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3309 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3312 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3313 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3315 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3317 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3319 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:24 WARN TaskSetManager: Stage 3321 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3324 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3325 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3327 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3329 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3331 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3333 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3336 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3337 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3339 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3341 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3343 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3345 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3348 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3349 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3351 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3353 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3355 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3357 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3360 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3361 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3363 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:25 WARN TaskSetManager: Stage 3365 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3367 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3369 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3372 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3373 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3375 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3377 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3379 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3381 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3383 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3386 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3387 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3389 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3391 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3393 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3395 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3397 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3400 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3401 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3403 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:26 WARN TaskSetManager: Stage 3405 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3407 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3409 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3411 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3414 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3415 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3417 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3419 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3421 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3423 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3425 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3428 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3429 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3431 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3433 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3435 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3437 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3439 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3442 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3443 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3445 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3447 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3449 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3451 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:27 WARN TaskSetManager: Stage 3453 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3455 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3458 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3459 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3461 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3463 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3465 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3467 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3469 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3471 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3474 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3475 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3477 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3479 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3481 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3483 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3485 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3487 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3490 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3491 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3493 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3495 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3497 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3499 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3501 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:28 WARN TaskSetManager: Stage 3503 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3506 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3507 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3509 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3511 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3513 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3515 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3517 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3519 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3522 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3523 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3525 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3527 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3529 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3531 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3533 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3535 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3537 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3540 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3541 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3543 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3545 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3547 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3549 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3551 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:29 WARN TaskSetManager: Stage 3553 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3555 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3558 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3559 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3561 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3563 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3565 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3567 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3569 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3571 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3573 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3576 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3577 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3579 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3581 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3583 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3585 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3587 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3589 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3591 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3594 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3595 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3597 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3599 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3601 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3603 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:30 WARN TaskSetManager: Stage 3605 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3607 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3609 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3612 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3613 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3615 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3617 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3619 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3621 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3623 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3625 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3627 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3629 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3632 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3633 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3635 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3637 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3639 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3641 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3643 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3645 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3647 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3649 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3652 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3653 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3655 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:31 WARN TaskSetManager: Stage 3657 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3659 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3661 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3663 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3665 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3667 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3669 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3672 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3673 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3675 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3677 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3679 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3681 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3683 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3685 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3687 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3689 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3692 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3693 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3695 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3697 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3699 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3701 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3703 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3705 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3707 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3709 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3712 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:32 WARN TaskSetManager: Stage 3713 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3715 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3717 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3719 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3721 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3723 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3725 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3727 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3729 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3731 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3734 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3735 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3737 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3739 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3741 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3743 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3745 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3747 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3749 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3751 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3753 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3756 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3757 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3759 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3761 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3763 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:33 WARN TaskSetManager: Stage 3765 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3767 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3769 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3771 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3773 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3775 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3778 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3779 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3781 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3783 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3785 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3787 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3789 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3791 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3793 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3795 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3797 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3800 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3801 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3803 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3805 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3807 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3809 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3811 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:34 WARN TaskSetManager: Stage 3813 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3815 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3817 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3819 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3822 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3823 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3825 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3827 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3829 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3831 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3833 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3835 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3837 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3839 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3841 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3843 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3846 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3847 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3849 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3851 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3853 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3855 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3857 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3859 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3861 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3863 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:35 WARN TaskSetManager: Stage 3865 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3867 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3870 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3871 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3873 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3875 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3877 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3879 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3881 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3883 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3885 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3887 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3889 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3891 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3894 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3895 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3897 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3899 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3901 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3903 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3905 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3907 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3909 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3911 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3913 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:36 WARN TaskSetManager: Stage 3915 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3918 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3919 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3921 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3923 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3925 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3927 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3929 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3931 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3933 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3935 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3937 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3939 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3942 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3943 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3945 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3947 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3949 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3951 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3953 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3955 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3957 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3959 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3961 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3963 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:37 WARN TaskSetManager: Stage 3965 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3968 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3969 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3971 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3973 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3975 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3977 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3979 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3981 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3983 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3985 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3987 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3989 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3991 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3994 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3995 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3997 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 3999 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4001 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4003 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4005 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4007 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4009 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4011 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4013 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:38 WARN TaskSetManager: Stage 4015 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4017 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4020 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4021 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4023 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4025 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4027 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4029 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4031 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4033 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4035 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4037 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4039 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4041 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4043 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4046 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4047 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4049 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4051 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4053 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4055 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4057 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4059 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4061 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4063 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:39 WARN TaskSetManager: Stage 4065 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4067 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4069 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4072 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4073 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4075 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4077 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4079 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4081 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4083 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4085 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:40 WARN TaskSetManager: Stage 4087 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:41 WARN TaskSetManager: Stage 4089 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:41 WARN TaskSetManager: Stage 4091 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:43:41 WARN TaskSetManager: Stage 4093 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        }
      ],
      "source": [
        "# fit the cv model with the assembled data\n",
        "cvModel = cv.fit(stocks_brent_gold_df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average RMSE:  9.57401754967\n"
          ]
        }
      ],
      "source": [
        "# get the average cross-validated RMSE\n",
        "# note the RMSE is the root of the average of the squares of the differences between the predicted and the actual values\n",
        "# see the documentation for more details: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator\n",
        "\n",
        "# the higher the RMSE, the worse the model\n",
        "# the lower the RMSE, the better the model\n",
        "# the RMSE is a measure of the quality of the model\n",
        "# the RMSE is scale-dependent\n",
        "# the RMSE can be used to compare different models\n",
        "# the RMSE can be used to compare different transformations of the same model\n",
        "# the RMSE can be used to compare different models on different datasets\n",
        "# the RMSE can be used to compare different models on the same dataset\n",
        "# the RMSE can be used to compare different models on the same dataset with different target variables\n",
        "# the RMSE can be used to compare different models on the same dataset with the same target variable\n",
        "# the RMSE can be used to compare different models on the same dataset with the same target variable and different features\n",
        "# the RMSE can be used to compare different models on the same dataset with the same target variable and the same features\n",
        "# the RMSE can be used to compare different models on the same dataset with the same target variable and the same features and different hyperparameters\n",
        "# the RMSE can be used to compare different models on the same dataset with the same target variable and the same features and the same hyperparameters\n",
        "# the RMSE value of 0 means the model is perfect\n",
        "# the RMSE value of 9 means the model is good?\n",
        "avg_rmse = np.mean(cvModel.avgMetrics)\n",
        "print(\"Average RMSE: \", avg_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='cacheNodeIds', doc='If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval.'): False, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='checkpointInterval', doc='set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext.'): 10, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='featuresCol', doc='features column name.'): 'features', Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: variance'): 'variance', Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='labelCol', doc='label column name.'): 'StockIndex', Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='leafCol', doc='Leaf indices column name. Predicted leaf index of each instance in each tree by preorder.'): '', Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='maxBins', doc='Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature.'): 80, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 10, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='maxMemoryInMB', doc='Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size.'): 256, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='minInfoGain', doc='Minimum information gain for a split to be considered at a tree node.'): 0.0, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 1, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='minWeightFractionPerNode', doc='Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5).'): 0.0, Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='DecisionTreeRegressor_72b1e044f2a8', name='seed', doc='random seed.'): -1348306839468353918}\n"
          ]
        }
      ],
      "source": [
        "# print all the parameters of the best model\n",
        "print(cvModel.bestModel.extractParamMap())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.522291912561494\n",
            "10.522291912561494\n",
            "10.521084551055708\n",
            "10.51280981321889\n",
            "10.513402909162403\n",
            "10.105402825818217\n",
            "10.104516314479561\n",
            "10.078112506357273\n",
            "10.068864457527516\n",
            "10.06684320937565\n",
            "9.892983968287458\n",
            "9.889723276827738\n",
            "9.828495107838203\n",
            "9.821123434486285\n",
            "9.813828512329861\n",
            "9.71373768243144\n",
            "9.707899114952871\n",
            "9.635758587698248\n",
            "9.627275129878342\n",
            "9.605273675223932\n",
            "9.565798877735235\n",
            "9.5341368758327\n",
            "9.470339779137046\n",
            "9.464775168352896\n",
            "9.459239826354809\n",
            "9.44437240344195\n",
            "9.382796425186196\n",
            "9.330419733140833\n",
            "9.32327721967102\n",
            "9.309536673126363\n",
            "9.318386259705175\n",
            "9.227781673043385\n",
            "9.180582890095703\n",
            "9.153743129328205\n",
            "9.16717438667466\n",
            "9.186272620448268\n",
            "9.080836043516497\n",
            "9.040114764571014\n",
            "8.989954591034191\n",
            "9.007446180757652\n",
            "9.064959314858148\n",
            "8.945241550955789\n",
            "8.904468834086398\n",
            "8.852746870645689\n",
            "8.874668741377592\n"
          ]
        }
      ],
      "source": [
        "# print the RMSE of all the models, just to view the difference in the cross-validated RMSE\n",
        "for rmse in cvModel.avgMetrics:\n",
        "    print(rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# plot all the RMSE values of the models to see the difference in the cross-validated RMSE\n",
        "plt.plot(cvModel.avgMetrics)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('RMSE')\n",
        "plt.title('RMSE of the models')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Feature  Importance\n",
            "0  trading_name     0.437826\n",
            "5            low    0.280224\n",
            "3           open    0.112374\n",
            "4           high    0.077254\n",
            "1         Sector    0.069495\n",
            "2           Date    0.022828\n"
          ]
        }
      ],
      "source": [
        "# print the feature importances\n",
        "\n",
        "# The feature importances represent the importance of each feature in the decision tree model.\n",
        "\n",
        "# The feature importances are calculated based on the information gain provided by each feature in splitting the data.\n",
        "\n",
        "# The higher the feature importance, the more important the feature is in making predictions.\n",
        "\n",
        "\n",
        "\n",
        "# get the feature importances\n",
        "feature_importances = cvModel.bestModel.featureImportances\n",
        "\n",
        "# create a DataFrame to display the feature importances\n",
        "feature_importances_df = pd.DataFrame(list(zip(stocks_brent_gold_df_spark.columns[2:], feature_importances)),\n",
        "                                      columns=[\"Feature\", \"Importance\"])\n",
        "\n",
        "# sort the DataFrame by feature importance in descending order\n",
        "feature_importances_df = feature_importances_df.sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "# display the feature importances\n",
        "print(feature_importances_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Feature  Importance\n",
            "0  Stock_Price    0.437826\n",
            "1  Brent_Price    0.069495\n",
            "2   Gold_Price    0.022828\n"
          ]
        }
      ],
      "source": [
        "# print the feature importances\n",
        "# Feature importances describe the relative importance of each feature in making accurate predictions.\n",
        "# The higher the value, the more important the feature is.\n",
        "# Feature importances can help you understand which features are most influential in your model's predictions.\n",
        "# Feature importances can guide feature selection and model interpretation.\n",
        "\n",
        "# get the feature importances\n",
        "feature_importances = cvModel.bestModel.featureImportances\n",
        "\n",
        "# create a pandas DataFrame to display the feature importances\n",
        "feature_importances_df = pd.DataFrame(list(zip([\"Stock_Price\", \"Brent_Price\", \"Gold_Price\"], feature_importances)),\n",
        "                                      columns=[\"Feature\", \"Importance\"])\n",
        "\n",
        "# display the feature importances\n",
        "print(feature_importances_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "80\n"
          ]
        }
      ],
      "source": [
        "# print the hyperparameters of the best model\n",
        "# The hyperparameters of the best model can provide insights into the configuration that yielded the best performance.\n",
        "# Understanding the hyperparameters can help you tune future models and optimize their performance.\n",
        "# The hyperparameters of the best model can guide further experimentation and refinement of the model.\n",
        "print(cvModel.bestModel._java_obj.getMaxDepth())\n",
        "print(cvModel.bestModel._java_obj.getMaxBins())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+-----------+------------------+\n",
            "|               Date|Stock_Price|        prediction|\n",
            "+-------------------+-----------+------------------+\n",
            "|2020-01-23 00:00:00|       90.9| 9.173453996983408|\n",
            "|2020-01-22 00:00:00|       91.8|12.083710407239819|\n",
            "|2020-01-21 00:00:00|       92.0| 9.173453996983408|\n",
            "|2020-01-20 00:00:00|       92.9|12.083710407239819|\n",
            "|2020-01-16 00:00:00|       92.8| 9.173453996983408|\n",
            "|2020-01-15 00:00:00|       92.4| 9.173453996983408|\n",
            "|2020-01-14 00:00:00|       93.2| 9.173453996983408|\n",
            "|2020-01-13 00:00:00|       92.7| 9.173453996983408|\n",
            "|2020-01-09 00:00:00|       92.0| 9.173453996983408|\n",
            "|2020-01-08 00:00:00|       90.5| 9.173453996983408|\n",
            "|2020-01-07 00:00:00|       91.0| 9.173453996983408|\n",
            "|2020-01-06 00:00:00|       91.9| 9.173453996983408|\n",
            "|2020-01-02 00:00:00|       93.4| 9.173453996983408|\n",
            "|2020-01-01 00:00:00|       93.1| 9.173453996983408|\n",
            "|2019-12-31 00:00:00|       93.9| 9.173453996983408|\n",
            "|2019-12-30 00:00:00|       93.7| 2.935483870967742|\n",
            "|2019-12-26 00:00:00|       93.3| 9.173453996983408|\n",
            "|2019-12-25 00:00:00|       92.1| 9.173453996983408|\n",
            "|2019-12-24 00:00:00|       93.0| 9.173453996983408|\n",
            "|2019-12-23 00:00:00|       93.2|3.0135379061371843|\n",
            "+-------------------+-----------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 15:50:55 WARN TaskSetManager: Stage 4096 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:50:55 WARN TaskSetManager: Stage 4097 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create a table for SABIC actual and predicted stock prices\n",
        "\n",
        "# get the predictions\n",
        "predictions = cvModel.transform(stocks_brent_gold_df_spark)\n",
        "\n",
        "# select the relevant columns for SABIC stock prices\n",
        "sabic_predictions = predictions.select(\"Date\", \"Stock_Price\", \"prediction\").filter(predictions.name == \"Saudi Basic Industries Corp.\")\n",
        "\n",
        "# display the actual and predicted stock prices for SABIC\n",
        "sabic_predictions.show()\n",
        "\n",
        "# plot the actual and predicted stock prices for SABIC\n",
        "sns.lineplot(data=sabic_predictions.toPandas(), x=\"Date\", y=\"Stock_Price\", label=\"Actual Stock Price\")\n",
        "sns.lineplot(data=sabic_predictions.toPandas(), x=\"Date\", y=\"prediction\", label=\"Predicted Stock Price\")\n",
        "plt.title(\"Actual vs. Predicted Stock Prices for SABIC\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Stock Price\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- we notice that the DecisionTreeRegressor model is not performing well; this can be due to the fact that the cross-validation is not respecting the time in the time-series data\n",
        "\n",
        "- we can try to use the test:train split method instead of cross-validation to verify if it yeilds better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 15:57:40 WARN TaskSetManager: Stage 4099 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:40 WARN TaskSetManager: Stage 4100 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:40 WARN TaskSetManager: Stage 4102 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:41 WARN TaskSetManager: Stage 4104 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:41 WARN TaskSetManager: Stage 4106 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:41 WARN TaskSetManager: Stage 4108 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:41 WARN TaskSetManager: Stage 4110 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:57:41 WARN TaskSetManager: Stage 4112 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 9.590728564371908\n"
          ]
        }
      ],
      "source": [
        "# split the data into training and testing\n",
        "# 80% of the data is used for training and 20% for testing\n",
        "(trainingData, testData) = stocks_brent_gold_df_spark.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the DecisionTreeRegressor\n",
        "dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"StockIndex\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = dt.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"StockIndex\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4115 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4116 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4117 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4119 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4121 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4123 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4125 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:48 WARN TaskSetManager: Stage 4127 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 15:59:49 WARN TaskSetManager: Stage 4129 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 12.903686166376568\n"
          ]
        }
      ],
      "source": [
        "# we do non-random split to respect the time in the time-series data\n",
        "# we split the data by date\n",
        "# the training data is before 2018 and the testing data is after 2018\n",
        "trainingData2 = stocks_brent_gold_df_spark.filter(stocks_brent_gold_df_spark.Date < '2018-01-01')\n",
        "testData2 = stocks_brent_gold_df_spark.filter(stocks_brent_gold_df_spark.Date >= '2018-01-01')\n",
        "\n",
        "# Define the DecisionTreeRegressor\n",
        "dt2 = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"StockIndex\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model2 = dt.fit(trainingData2)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions2 = model2.transform(testData2)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator2 = RegressionEvaluator(labelCol=\"StockIndex\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse2 = evaluator2.evaluate(predictions2)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- from above, we see Regression isn't yeilding good results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.3 Model Selection: GBTRgressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gradient-Boosted Trees (GBTs) learning algorithm for regression. It supports both continuous and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# before we try different models, we need to preprocess the data again\n",
        "stocks_brent_gold_df = preprocess_data(stocks_df, brent_df, gold_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# because we are going to use the DecisionTreeRegressor, we need to convert the categorical data to numerical data\n",
        "indexer = StringIndexer(inputCol=\"Sector\", outputCol=\"SectorIndex\")\n",
        "indexed = indexer.fit(stocks_brent_gold_df_spark).transform(stocks_brent_gold_df_spark)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# because we are going to use the DecisionTreeRegressor, we need to convert the categorical data to numerical data\n",
        "indexer = StringIndexer(inputCol=\"Sector\", outputCol=\"SectorIndex\")\n",
        "indexed = stocks_brent_gold_df_spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "# change StockIndex to label\n",
        "indexed = indexed.withColumnRenamed(\"StockIndex\", \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "|symbol|                name|  trading_name |   Sector|               Date| open| high|  low|Stock_Price|change|perc_Change|volume_traded |  value_traded|no_trades |Brent_Price|Gold_Price|SectorIndex|label|            features|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "|  2030|Saudi Arabia Refi...|          SARCO|   Energy|2020-03-05 00:00:00|35.55|35.85| 34.9|       34.9|  -0.4|      -1.13|      436609.0|  1.53990735E7|     804.0|      51.29|    1690.5|        1.0| 18.0|[34.9,436609.0,1....|\n",
            "|  2222|Saudi Arabian Oil...|   SAUDI ARAMCO|   Energy|2020-03-05 00:00:00|33.05| 33.2|32.95|       33.0| -0.05|      -0.15|     3969243.0|1.3103890695E8|    4576.0|      51.29|    1690.5|        1.0| 46.0|[33.0,3969243.0,1...|\n",
            "|  2380|Rabigh Refining a...|   PETRO RABIGH|   Energy|2020-03-05 00:00:00|15.18|15.18| 14.8|       14.8| -0.26|      -1.73|     1328273.0|  1.98755901E7|    1370.0|      51.29|    1690.5|        1.0| 28.0|[14.8,1328273.0,1...|\n",
            "|  4030|National Shipping...|          BAHRI|   Energy|2020-03-05 00:00:00|31.95| 32.0| 30.8|      31.15| -0.55|      -1.74|      597076.0| 1.875434865E7|     796.0|      51.29|    1690.5|        1.0|  1.0|[31.15,597076.0,1...|\n",
            "|  4200|Aldrees Petroleum...|        ALDREES|   Energy|2020-03-05 00:00:00| 60.3| 61.3| 60.1|       60.1|   0.1|       0.17|      350180.0|  2.12106028E7|     734.0|      51.29|    1690.5|        1.0| 23.0|[60.1,350180.0,2....|\n",
            "|  1201|Takween Advanced ...|        TAKWEEN|Materials|2020-03-05 00:00:00| 7.68| 7.68|  7.5|       7.52| -0.04|      -0.53|      183425.0|     1386959.3|     275.0|      51.29|    1690.5|        0.0| 37.0|[7.52,183425.0,13...|\n",
            "|  1202|Middle East Paper...|          MEPCO|Materials|2020-03-05 00:00:00|13.66| 13.8|13.54|      13.54| -0.02|      -0.15|      584937.0|    7975687.54|     570.0|      51.29|    1690.5|        0.0| 42.0|[13.54,584937.0,7...|\n",
            "|  1210|Basic Chemical In...|            BCI|Materials|2020-03-05 00:00:00| 24.1| 24.3|23.84|       23.9| -0.06|      -0.25|      116835.0|    2807511.36|     218.0|      51.29|    1690.5|        0.0| 29.0|[23.9,116835.0,28...|\n",
            "|  1211|Saudi Arabian Min...|         MAADEN|Materials|2020-03-05 00:00:00| 36.0|36.35| 34.8|       34.8|  -0.8|      -2.25|      711699.0|  2.53748644E7|    1197.0|      51.29|    1690.5|        0.0| 30.0|[34.8,711699.0,2....|\n",
            "|  1301|United Wire Facto...|          ASLAK|Materials|2020-03-05 00:00:00| 19.5|19.56| 19.0|      19.04| -0.32|      -1.65|       76580.0|    1471655.68|     116.0|      51.29|    1690.5|        0.0| 35.0|[19.04,76580.0,14...|\n",
            "|  1304|Al Yamamah Steel ...|ALYAMAMAH STEEL|Materials|2020-03-05 00:00:00|20.02|20.12|19.62|       19.8| -0.18|       -0.9|      134742.0|    2671081.78|     243.0|      51.29|    1690.5|        0.0| 43.0|[19.8,134742.0,26...|\n",
            "|  1320|Saudi Steel Pipe Co.|            SSP|Materials|2020-03-05 00:00:00| 18.9| 19.1|18.56|      18.62|  0.06|       0.32|      241445.0|    4560501.24|     504.0|      51.29|    1690.5|        0.0| 32.0|[18.62,241445.0,4...|\n",
            "|  2001|Methanol Chemical...|       CHEMANOL|Materials|2020-03-05 00:00:00| 7.55| 7.57|  7.4|        7.4|  -0.1|      -1.33|      639345.0|    4798391.69|     585.0|      51.29|    1690.5|        0.0| 31.0|[7.4,639345.0,479...|\n",
            "|  2002|National Petroche...|      PETROCHEM|Materials|2020-03-05 00:00:00| 22.6| 22.7|22.56|      22.56| -0.12|      -0.53|     1746535.0| 3.940379092E7|     333.0|      51.29|    1690.5|        0.0| 33.0|[22.56,1746535.0,...|\n",
            "|  2020|Saudi Arabian Fer...|          SAFCO|Materials|2020-03-05 00:00:00| 68.4| 69.1| 67.4|       67.5|  -0.8|      -1.17|      191727.0|  1.30549484E7|     634.0|      51.29|    1690.5|        0.0|  2.0|[67.5,191727.0,1....|\n",
            "|  2060|National Industri...|         TASNEE|Materials|2020-03-05 00:00:00|11.54|11.64| 11.4|      11.42| -0.08|       -0.7|     1076342.0| 1.241573444E7|     833.0|      51.29|    1690.5|        0.0|  0.0|[11.42,1076342.0,...|\n",
            "|  2090| National Gypsum Co.|            NGC|Materials|2020-03-05 00:00:00|14.74|14.92|14.54|      14.58| -0.08|      -0.55|      402940.0|    5932614.32|     547.0|      51.29|    1690.5|        0.0| 14.0|[14.58,402940.0,5...|\n",
            "|  2150|The National Comp...|         ZOUJAJ|Materials|2020-03-05 00:00:00|16.02| 16.1|15.82|      15.82|  -0.1|      -0.63|       58023.0|     926556.96|     125.0|      51.29|    1690.5|        0.0| 17.0|[15.82,58023.0,92...|\n",
            "|  2170|Alujain Holding C...|        ALUJAIN|Materials|2020-03-05 00:00:00| 6.45|  7.1| 6.45|       33.0|   0.0|        0.0|           0.0|           0.0|       0.0|      51.29|    1690.5|        0.0|  7.0|[33.0,0.0,0.0,0.0...|\n",
            "|  2180|Filing and Packin...|          FIPCO|Materials|2020-03-05 00:00:00| 38.8|39.75| 37.9|       39.2|   0.8|       2.08|     1112098.0|  4.32869109E7|    1666.0|      51.29|    1690.5|        0.0| 19.0|[39.2,1112098.0,4...|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 16:23:31 WARN TaskSetManager: Stage 4136 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:31 WARN TaskSetManager: Stage 4137 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4139 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4141 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4143 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4145 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4147 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4149 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4151 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4153 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4155 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4157 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4159 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4161 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4163 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4165 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4167 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4169 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4171 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4173 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:32 WARN TaskSetManager: Stage 4175 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4177 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4179 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4181 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4183 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4185 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4187 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4189 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4191 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4193 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4195 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4197 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4199 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4201 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4203 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4205 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4207 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4209 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4211 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4213 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4215 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4217 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4219 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4221 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4223 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:33 WARN TaskSetManager: Stage 4225 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4227 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4229 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4231 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4233 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4235 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4237 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4239 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4241 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4243 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4245 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4247 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4249 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4251 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4253 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4255 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4257 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4259 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4261 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4263 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4265 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4267 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4269 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4271 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:34 WARN TaskSetManager: Stage 4273 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4275 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4277 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4279 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4281 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4283 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4285 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4287 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4289 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4291 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4293 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4295 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4297 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4299 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4301 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4303 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4305 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4307 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4309 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4311 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4313 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4315 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4317 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4319 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4321 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:35 WARN TaskSetManager: Stage 4323 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4325 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4327 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4329 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4331 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4333 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4335 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4337 contains a task of very large size (1103 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 16:23:36 WARN TaskSetManager: Stage 4339 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 9.01677305623062\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 16:23:36 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
          ]
        }
      ],
      "source": [
        "# implement a model that predicts the stock price of SABIC\n",
        "# we have various models to choose from, such as DecisionTreeRegressor, RandomForestRegressor, GBTRegressor, etc.\n",
        "# each model has its own strengths and weaknesses\n",
        "# we can use the GBTRgressor model to predict the stock price of SABIC\n",
        "# the GBTRegressor model is an ensemble model that uses multiple decision trees to make predictions\n",
        "# the GBTRegressor model is a powerful model that can capture complex relationships in the data\n",
        "# the GBTRegressor model is a popular model for regression problems\n",
        "# the GBTRegressor model is a good choice for predicting the stock price of SABIC\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "# split the data into training and testing\n",
        "# 80% of the data is used for training and 20% for testing\n",
        "(trainingData, testData) = indexed.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the GBTRegressor\n",
        "gbt = GBTRegressor(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = gbt.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 18:26:37 WARN Instrumentation: [5d6f5b9d] regParam is zero, which might cause numerical instability and overfitting.\n",
            "24/04/29 18:26:37 WARN TaskSetManager: Stage 4353 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Root Mean Squared Error (RMSE): 10.802688378472856\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 18:26:38 WARN TaskSetManager: Stage 4355 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n"
          ]
        }
      ],
      "source": [
        "# implement a Stochastic Gradient Descent - SGD\n",
        "# split the data into training and testing\n",
        "# 80% of the data is used for training and 20% for testing\n",
        "(trainingData, testData) = indexed.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the Stochastic Gradient Descent - SGD\n",
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "sgd = GeneralizedLinearRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = sgd.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE):\", rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2.4 Model Selection: SVMModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "|symbol|                name|  trading_name |   Sector|               Date| open| high|  low|Stock_Price|change|perc_Change|volume_traded |  value_traded|no_trades |Brent_Price|Gold_Price|SectorIndex|label|            features|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "|  2030|Saudi Arabia Refi...|          SARCO|   Energy|2020-03-05 00:00:00|35.55|35.85| 34.9|       34.9|  -0.4|      -1.13|      436609.0|  1.53990735E7|     804.0|      51.29|    1690.5|        1.0| 18.0|[34.9,436609.0,1....|\n",
            "|  2222|Saudi Arabian Oil...|   SAUDI ARAMCO|   Energy|2020-03-05 00:00:00|33.05| 33.2|32.95|       33.0| -0.05|      -0.15|     3969243.0|1.3103890695E8|    4576.0|      51.29|    1690.5|        1.0| 46.0|[33.0,3969243.0,1...|\n",
            "|  2380|Rabigh Refining a...|   PETRO RABIGH|   Energy|2020-03-05 00:00:00|15.18|15.18| 14.8|       14.8| -0.26|      -1.73|     1328273.0|  1.98755901E7|    1370.0|      51.29|    1690.5|        1.0| 28.0|[14.8,1328273.0,1...|\n",
            "|  4030|National Shipping...|          BAHRI|   Energy|2020-03-05 00:00:00|31.95| 32.0| 30.8|      31.15| -0.55|      -1.74|      597076.0| 1.875434865E7|     796.0|      51.29|    1690.5|        1.0|  1.0|[31.15,597076.0,1...|\n",
            "|  4200|Aldrees Petroleum...|        ALDREES|   Energy|2020-03-05 00:00:00| 60.3| 61.3| 60.1|       60.1|   0.1|       0.17|      350180.0|  2.12106028E7|     734.0|      51.29|    1690.5|        1.0| 23.0|[60.1,350180.0,2....|\n",
            "|  1201|Takween Advanced ...|        TAKWEEN|Materials|2020-03-05 00:00:00| 7.68| 7.68|  7.5|       7.52| -0.04|      -0.53|      183425.0|     1386959.3|     275.0|      51.29|    1690.5|        0.0| 37.0|[7.52,183425.0,13...|\n",
            "|  1202|Middle East Paper...|          MEPCO|Materials|2020-03-05 00:00:00|13.66| 13.8|13.54|      13.54| -0.02|      -0.15|      584937.0|    7975687.54|     570.0|      51.29|    1690.5|        0.0| 42.0|[13.54,584937.0,7...|\n",
            "|  1210|Basic Chemical In...|            BCI|Materials|2020-03-05 00:00:00| 24.1| 24.3|23.84|       23.9| -0.06|      -0.25|      116835.0|    2807511.36|     218.0|      51.29|    1690.5|        0.0| 29.0|[23.9,116835.0,28...|\n",
            "|  1211|Saudi Arabian Min...|         MAADEN|Materials|2020-03-05 00:00:00| 36.0|36.35| 34.8|       34.8|  -0.8|      -2.25|      711699.0|  2.53748644E7|    1197.0|      51.29|    1690.5|        0.0| 30.0|[34.8,711699.0,2....|\n",
            "|  1301|United Wire Facto...|          ASLAK|Materials|2020-03-05 00:00:00| 19.5|19.56| 19.0|      19.04| -0.32|      -1.65|       76580.0|    1471655.68|     116.0|      51.29|    1690.5|        0.0| 35.0|[19.04,76580.0,14...|\n",
            "|  1304|Al Yamamah Steel ...|ALYAMAMAH STEEL|Materials|2020-03-05 00:00:00|20.02|20.12|19.62|       19.8| -0.18|       -0.9|      134742.0|    2671081.78|     243.0|      51.29|    1690.5|        0.0| 43.0|[19.8,134742.0,26...|\n",
            "|  1320|Saudi Steel Pipe Co.|            SSP|Materials|2020-03-05 00:00:00| 18.9| 19.1|18.56|      18.62|  0.06|       0.32|      241445.0|    4560501.24|     504.0|      51.29|    1690.5|        0.0| 32.0|[18.62,241445.0,4...|\n",
            "|  2001|Methanol Chemical...|       CHEMANOL|Materials|2020-03-05 00:00:00| 7.55| 7.57|  7.4|        7.4|  -0.1|      -1.33|      639345.0|    4798391.69|     585.0|      51.29|    1690.5|        0.0| 31.0|[7.4,639345.0,479...|\n",
            "|  2002|National Petroche...|      PETROCHEM|Materials|2020-03-05 00:00:00| 22.6| 22.7|22.56|      22.56| -0.12|      -0.53|     1746535.0| 3.940379092E7|     333.0|      51.29|    1690.5|        0.0| 33.0|[22.56,1746535.0,...|\n",
            "|  2020|Saudi Arabian Fer...|          SAFCO|Materials|2020-03-05 00:00:00| 68.4| 69.1| 67.4|       67.5|  -0.8|      -1.17|      191727.0|  1.30549484E7|     634.0|      51.29|    1690.5|        0.0|  2.0|[67.5,191727.0,1....|\n",
            "|  2060|National Industri...|         TASNEE|Materials|2020-03-05 00:00:00|11.54|11.64| 11.4|      11.42| -0.08|       -0.7|     1076342.0| 1.241573444E7|     833.0|      51.29|    1690.5|        0.0|  0.0|[11.42,1076342.0,...|\n",
            "|  2090| National Gypsum Co.|            NGC|Materials|2020-03-05 00:00:00|14.74|14.92|14.54|      14.58| -0.08|      -0.55|      402940.0|    5932614.32|     547.0|      51.29|    1690.5|        0.0| 14.0|[14.58,402940.0,5...|\n",
            "|  2150|The National Comp...|         ZOUJAJ|Materials|2020-03-05 00:00:00|16.02| 16.1|15.82|      15.82|  -0.1|      -0.63|       58023.0|     926556.96|     125.0|      51.29|    1690.5|        0.0| 17.0|[15.82,58023.0,92...|\n",
            "|  2170|Alujain Holding C...|        ALUJAIN|Materials|2020-03-05 00:00:00| 6.45|  7.1| 6.45|       33.0|   0.0|        0.0|           0.0|           0.0|       0.0|      51.29|    1690.5|        0.0|  7.0|[33.0,0.0,0.0,0.0...|\n",
            "|  2180|Filing and Packin...|          FIPCO|Materials|2020-03-05 00:00:00| 38.8|39.75| 37.9|       39.2|   0.8|       2.08|     1112098.0|  4.32869109E7|    1666.0|      51.29|    1690.5|        0.0| 19.0|[39.2,1112098.0,4...|\n",
            "+------+--------------------+---------------+---------+-------------------+-----+-----+-----+-----------+------+-----------+--------------+--------------+----------+-----------+----------+-----------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# drop the label column\n",
        "indexed = indexed.drop(\"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:53:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:53:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:53:56 WARN TaskSetManager: Stage 4360 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:53:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:53:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        }
      ],
      "source": [
        "# add a new column to the DataFrame that indicates if the stock price increased or decreased from the previous day, with label 1 for increase and 0 for decrease\n",
        "# this column will be used as the label for the model\n",
        "indexed = indexed.withColumn(\"Price_Change\",\n",
        "                        F.when(F.col(\"Stock_Price\") > F.lag(\"Stock_Price\").over(Window.orderBy(\"Date\")), 1)\n",
        "                        .otherwise(0))\n",
        "\n",
        "# Convert Price_Change to a numerical label\n",
        "label_indexer = StringIndexer(inputCol=\"Price_Change\", outputCol=\"label\")\n",
        "indexed = label_indexer.fit(indexed).transform(indexed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# rename Price_Change column to label\n",
        "indexed = indexed.withColumnRenamed(\"Price_Change\", \"label\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------+--------------------+--------------+---------+-------------------+-----+-----+-----+-----------+-------+-----------+--------------+-------------+----------+-----------+----------+-----------+--------------------+-----+\n",
            "|symbol|                name| trading_name |   Sector|               Date| open| high|  low|Stock_Price| change|perc_Change|volume_traded | value_traded|no_trades |Brent_Price|Gold_Price|SectorIndex|            features|label|\n",
            "+------+--------------------+--------------+---------+-------------------+-----+-----+-----+-----------+-------+-----------+--------------+-------------+----------+-----------+----------+-----------+--------------------+-----+\n",
            "|  4030|National Shipping...|         BAHRI|   Energy|2001-12-31 00:00:00| 9.25| 9.25| 9.25|       9.25|   9.25|        0.0|      256935.0|    2367154.5|      44.0|      19.35|     278.7|        1.0|[9.25,256935.0,23...|    0|\n",
            "|  2010|Saudi Basic Indus...|         SABIC|Materials|2001-12-31 00:00:00|15.75|15.75| 15.5|       15.5|   15.5|        0.0|      475546.0|   7425540.25|      74.0|      19.35|     278.7|        0.0|[15.5,475546.0,74...|    1|\n",
            "|  2020|Saudi Arabian Fer...|         SAFCO|Materials|2001-12-31 00:00:00| 15.3| 15.3|15.15|      15.16|-143.84|     -90.47|      738034.0|1.121877125E7|     108.0|      19.35|     278.7|        0.0|[15.16,738034.0,1...|    0|\n",
            "|  2060|National Industri...|        TASNEE|Materials|2001-12-31 00:00:00|  2.4|  2.4| 2.25|        2.4|    2.4|        0.0|      609863.0|    1401048.5|      31.0|      19.35|     278.7|        0.0|[2.4,609863.0,140...|    0|\n",
            "|  2090| National Gypsum Co.|           NGC|Materials|2001-12-31 00:00:00| 27.0|28.25|28.25|      28.25|  28.25|        0.0|         960.0|      26804.0|       2.0|      19.35|     278.7|        0.0|[28.25,960.0,2680...|    1|\n",
            "|  2150|The National Comp...|        ZOUJAJ|Materials|2001-12-31 00:00:00| 6.45|  7.1| 6.45|        7.1|    7.1|        0.0|        8266.0|      55256.0|       3.0|      19.35|     278.7|        0.0|[7.1,8266.0,55256...|    0|\n",
            "|  2170|Alujain Holding C...|       ALUJAIN|Materials|2001-12-31 00:00:00|  2.5|  2.5| 2.25|       2.25|   2.25|        0.0|       56665.0|     134162.5|       6.0|      19.35|     278.7|        0.0|[2.25,56665.0,134...|    0|\n",
            "|  2210|  Nama Chemicals Co.|NAMA CHEMICALS|Materials|2001-12-31 00:00:00|10.95|10.95|10.95|      10.93| -18.32|     -62.63|        4539.0|      49800.0|       2.0|      19.35|     278.7|        0.0|[10.93,4539.0,498...|    1|\n",
            "|  3010|  Arabian Cement Co.|           ACC|Materials|2001-12-31 00:00:00| 24.2| 24.2| 24.2|       24.2|   24.2|        0.0|      138191.0|    3338202.0|      34.0|      19.35|     278.7|        0.0|[24.2,138191.0,33...|    1|\n",
            "|  3020|   Yamama Cement Co.|          YSCC|Materials|2001-12-31 00:00:00|22.85| 23.0|22.85|      22.85|  22.85|        0.0|       67523.0|    1544363.0|      16.0|      19.35|     278.7|        0.0|[22.85,67523.0,15...|    0|\n",
            "|  3030|    Saudi Cement Co.|  SAUDI CEMENT|Materials|2001-12-31 00:00:00| 26.0| 26.2| 25.8|       26.2|   26.2|        0.0|      197108.0|   5124748.75|      45.0|      19.35|     278.7|        0.0|[26.2,197108.0,51...|    1|\n",
            "|  3040|   Qassim Cement Co.|         QACCO|Materials|2001-12-31 00:00:00| 36.8| 36.9| 36.5|       36.9|   36.9|        0.0|       98440.0|   3608721.25|      22.0|      19.35|     278.7|        0.0|[36.9,98440.0,360...|    1|\n",
            "|  3050|Southern Province...|          SPCC|Materials|2001-12-31 00:00:00|45.25|45.25|44.75|       45.0|   45.0|        0.0|       84665.0|    3813375.0|      12.0|      19.35|     278.7|        0.0|[45.0,84665.0,381...|    1|\n",
            "|  3060|    Yanbu Cement Co.|           YCC|Materials|2001-12-31 00:00:00| 22.5|22.65|22.35|      22.35|  22.35|        0.0|      460598.0|  1.0325333E7|      82.0|      19.35|     278.7|        0.0|[22.35,460598.0,1...|    0|\n",
            "|  3080|Eastern Province ...|         EPCCO|Materials|2001-12-31 00:00:00|40.25|40.25| 40.0|       40.0|   40.0|        0.0|     1592825.0| 6.38116155E7|     102.0|      19.35|     278.7|        0.0|[40.0,1592825.0,6...|    1|\n",
            "|  3090|    Tabuk Cement Co.|           TCC|Materials|2001-12-31 00:00:00| 13.5| 13.5| 13.5|       13.5|   13.5|        0.0|      191627.0|    2582733.5|      42.0|      19.35|     278.7|        0.0|[13.5,191627.0,25...|    0|\n",
            "|  4030|National Shipping...|         BAHRI|   Energy|2002-01-02 00:00:00| 9.25| 9.25|  9.0|       9.25|    0.0|        0.0|       30793.0|    281304.75|      12.0|      20.13|     278.9|        1.0|[9.25,30793.0,281...|    0|\n",
            "|  2010|Saudi Basic Indus...|         SABIC|Materials|2002-01-02 00:00:00|15.75|15.75| 15.5|      15.75|    0.0|        0.0|     1364333.0|2.135090225E7|     121.0|      20.13|     278.9|        0.0|[15.75,1364333.0,...|    1|\n",
            "|  2020|Saudi Arabian Fer...|         SAFCO|Materials|2002-01-02 00:00:00| 15.3| 15.3|15.15|      15.32|    0.0|        0.0|      417521.0|    6367952.5|      83.0|      20.13|     278.9|        0.0|[15.32,417521.0,6...|    0|\n",
            "|  2060|National Industri...|        TASNEE|Materials|2002-01-02 00:00:00|  2.4|  2.4|  2.4|        2.4|    0.0|        0.0|      496826.0|    1144497.0|      26.0|      20.13|     278.9|        0.0|[2.4,496826.0,114...|    0|\n",
            "+------+--------------------+--------------+---------+-------------------+-----+-----+-----+-----------+-------+-----------+--------------+-------------+----------+-----------+----------+-----------+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:55:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:27 WARN TaskSetManager: Stage 4369 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        }
      ],
      "source": [
        "indexed.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN TaskSetManager: Stage 4372 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN TaskSetManager: Stage 4373 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:34 WARN TaskSetManager: Stage 4670 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN TaskSetManager: Stage 4671 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN TaskSetManager: Stage 4672 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:55:35 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6846435960127891\n"
          ]
        }
      ],
      "source": [
        "from pyspark.mllib.linalg import SparseVector\n",
        "# implement the model using SVMModel\n",
        "# split the data into training and testing\n",
        "# 80% of the data is used for training and 20% for testing\n",
        "(trainingData, testData) = indexed.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Define the SVMModel\n",
        "from pyspark.ml.classification import LinearSVC\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = svm.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN TaskSetManager: Stage 4676 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN TaskSetManager: Stage 4677 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:26 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN TaskSetManager: Stage 5072 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN TaskSetManager: Stage 5073 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN TaskSetManager: Stage 5074 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:56:27 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6246453182002432\n"
          ]
        }
      ],
      "source": [
        "# split the data with the time-series in mind\n",
        "# we split the data by date\n",
        "# the training data is before 2018 and the testing data is after 2018\n",
        "trainingData = indexed.filter(indexed.Date < '2018-01-01')\n",
        "testData = indexed.filter(indexed.Date >= '2018-01-01')\n",
        "\n",
        "# Define the SVMModel\n",
        "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Train the model on the training data\n",
        "model = svm.fit(trainingData)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN TaskSetManager: Stage 5081 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of correct predictions: 12328\n",
            "Number of incorrect predictions: 7408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN TaskSetManager: Stage 5084 contains a task of very large size (1102 KiB). The maximum recommended task size is 1000 KiB.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
            "24/04/29 20:57:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
          ]
        }
      ],
      "source": [
        "# print number of correct predictions\n",
        "correct_predictions = predictions.filter(predictions.label == predictions.prediction).count()\n",
        "print(\"Number of correct predictions:\", correct_predictions)\n",
        "\n",
        "# print number of incorrect predictions\n",
        "incorrect_predictions = predictions.filter(predictions.label != predictions.prediction).count()\n",
        "print(\"Number of incorrect predictions:\", incorrect_predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "         Feature    Importance\n",
            "0    Stock_Price  4.711053e-02\n",
            "5      no_trades  7.807593e-04\n",
            "3  volume_traded  1.767314e-05\n",
            "1    Brent_Price  9.761599e-09\n",
            "2     Gold_Price -1.712160e-09\n",
            "4   value_traded -1.306478e-02\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "24/04/29 22:50:32 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 128500 ms exceeds timeout 120000 ms\n",
            "24/04/29 22:50:32 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
            "24/04/29 22:50:40 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:50:40 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:50:50 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:50:50 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:51:00 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:51:00 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:51:10 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 22:51:10 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 23:06:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 23:06:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 23:06:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:06:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:07:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:08:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:09:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:10:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:17 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:17 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:27 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:27 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:37 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:37 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:47 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:47 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:57 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:11:57 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:12:07 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:12:07 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:34 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:44 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:44 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:54 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:28:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:04 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:04 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:14 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:14 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:24 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:24 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:34 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:44 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:44 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:54 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:29:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:04 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:04 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:14 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:14 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:24 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:24 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:34 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:34 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:44 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:44 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:54 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:30:54 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:31:04 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:31:04 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:46:58 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:46:58 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:08 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:08 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:18 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 23:47:18 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallback(Promise.scala:316)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.onComplete(Promise.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.transformWith(Promise.scala:40)\n",
            "\tat scala.concurrent.impl.Promise.transformWith$(Promise.scala:38)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.transformWith(Promise.scala:187)\n",
            "\tat scala.concurrent.Future.flatMap(Future.scala:306)\n",
            "\tat scala.concurrent.Future.flatMap$(Future.scala:306)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:150)\n",
            "\t... 17 more\n",
            "24/04/29 23:47:28 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:28 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:38 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:38 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:48 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:48 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:58 ERROR Inbox: Ignoring error\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:58 WARN Executor: Issue communicating with driver in heartbeater\n",
            "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
            "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
            "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:80)\n",
            "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:642)\n",
            "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1223)\n",
            "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:295)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
            "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n",
            "\tat java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n",
            "\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
            "\tat java.lang.Thread.run(Thread.java:750)\n",
            "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
            "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
            "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
            "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
            "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:124)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:123)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:688)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:687)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:725)\n",
            "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:133)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:103)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
            "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
            "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
            "\t... 3 more\n",
            "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@192.168.100.154:59894\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)\n",
            "\tat scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.processBatch$1(BatchingExecutor.scala:67)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.$anonfun$run$1(BatchingExecutor.scala:82)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85)\n",
            "\tat scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:59)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875)\n",
            "\tat scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:110)\n",
            "\tat scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107)\n",
            "\tat scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.trySuccess(Promise.scala:94)\n",
            "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:94)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:239)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:238)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
            "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
            "\tat scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)\n",
            "\tat scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
            "\tat scala.concurrent.Promise.complete(Promise.scala:53)\n",
            "\tat scala.concurrent.Promise.complete$(Promise.scala:52)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:187)\n",
            "\tat scala.concurrent.Promise.success(Promise.scala:86)\n",
            "\tat scala.concurrent.Promise.success$(Promise.scala:86)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:187)\n",
            "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
            "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
            "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
            "\t... 8 more\n",
            "24/04/29 23:47:58 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n",
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 59946)\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
            "    self.handle()\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "                           ^^^^^^\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/Users/bandaralghamdi/anaconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# print the feature importances\n",
        "# Feature importances describe the relative importance of each feature in making accurate predictions.\n",
        "\n",
        "# The higher the value, the more important the feature is.\n",
        "\n",
        "# Feature importances can help you understand which features are most influential in your model's predictions.\n",
        "\n",
        "\n",
        "# get the feature importances\n",
        "feature_importances = model.coefficients\n",
        "\n",
        "# create a DataFrame to display the feature importances\n",
        "feature_importances_df = pd.DataFrame(list(zip([\"Stock_Price\", \"Brent_Price\", \"Gold_Price\", \"volume_traded\", \"value_traded\", \"no_trades\"], feature_importances)),\n",
        "                                      columns=[\"Feature\", \"Importance\"])\n",
        "\n",
        "# sort the DataFrame by feature importance in descending order\n",
        "feature_importances_df = feature_importances_df.sort_values(\"Importance\", ascending=False)\n",
        "\n",
        "# display the feature importances\n",
        "print(feature_importances_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\n\\nan RMSE of 9.841781224229344 means that, on average, the model's predictions are about 9.84 units away from the actual values.\\n\\nWhether this RMSE is considered good or bad depends on the context and the scale of your data. For example, if you are predicting stock prices that range from 100 to 200, an RMSE of 9.84 might be acceptable. However, if your stock prices range from 10 to 20, an RMSE of 9.84 would be quite high, indicating that your model's predictions are not very accurate.\\n\\nIt's also important to compare this RMSE to the performance of other models or benchmarks. If other models or approaches yield significantly lower RMSE values, then an RMSE of 9.84 might be considered suboptimal.\\n\\nConsidering your project involves predicting the exact prices of stocks, Brent oil, Ethereum, and gold, the evaluation of the RMSE value depends on the specific context:\\n\\nStock Prices: If the stock prices you're predicting have a wide range (e.g., $50 to $300), an RMSE of 9.84 might be considered reasonable. However, for lower-priced stocks or if you aim for high precision, this RMSE could be considered high.\\nBrent Oil Prices: The price range for Brent oil can vary significantly depending on market conditions. If the range is broad, an RMSE of 9.84 might be acceptable. However, for more stable periods with narrower price ranges, this value might be too high.\\nEthereum Prices: Given the volatility and wide range of cryptocurrency prices, an RMSE of 9.84 could be considered reasonable, but this depends on the specific price range during your prediction period.\\nGold Prices: Similar to Brent oil, the evaluation of the RMSE for gold prices depends on the price range during the period you're predicting. For a wide range, an RMSE of 9.84 might be acceptable, but for a narrower range, it could be considered high.\\nOverall, the acceptability of an RMSE of 9.84 in your project depends on the specific price ranges and volatility of the assets you're predicting. It's also important to compare this value to other models or benchmarks to evaluate its relative performance.\\n\\n\\n\""
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "an RMSE of 9.841781224229344 means that, on average, the model's predictions are about 9.84 units away from the actual values.\n",
        "\n",
        "Whether this RMSE is considered good or bad depends on the context and the scale of your data. For example, if you are predicting stock prices that range from 100 to 200, an RMSE of 9.84 might be acceptable. However, if your stock prices range from 10 to 20, an RMSE of 9.84 would be quite high, indicating that your model's predictions are not very accurate.\n",
        "\n",
        "It's also important to compare this RMSE to the performance of other models or benchmarks. If other models or approaches yield significantly lower RMSE values, then an RMSE of 9.84 might be considered suboptimal.\n",
        "\n",
        "Considering your project involves predicting the exact prices of stocks, Brent oil, Ethereum, and gold, the evaluation of the RMSE value depends on the specific context:\n",
        "\n",
        "Stock Prices: If the stock prices you're predicting have a wide range (e.g., $50 to $300), an RMSE of 9.84 might be considered reasonable. However, for lower-priced stocks or if you aim for high precision, this RMSE could be considered high.\n",
        "Brent Oil Prices: The price range for Brent oil can vary significantly depending on market conditions. If the range is broad, an RMSE of 9.84 might be acceptable. However, for more stable periods with narrower price ranges, this value might be too high.\n",
        "Ethereum Prices: Given the volatility and wide range of cryptocurrency prices, an RMSE of 9.84 could be considered reasonable, but this depends on the specific price range during your prediction period.\n",
        "Gold Prices: Similar to Brent oil, the evaluation of the RMSE for gold prices depends on the price range during the period you're predicting. For a wide range, an RMSE of 9.84 might be acceptable, but for a narrower range, it could be considered high.\n",
        "Overall, the acceptability of an RMSE of 9.84 in your project depends on the specific price ranges and volatility of the assets you're predicting. It's also important to compare this value to other models or benchmarks to evaluate its relative performance.\n",
        "\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
